[
  {
    "objectID": "approaches/dynamic-client.html",
    "href": "approaches/dynamic-client.html",
    "title": "Dynamic Client",
    "section": "",
    "text": "Traditional approaches to rendering raster datasets in the browser involve the creation of tiles in a pixelated image format like PNG or JPEG. These tiles can be pre-generated or created by a tiling server on demand, as described in the tiling approach chapter. While the image tiles are fast to load and easy to render, tiling offers limited flexibility for dynamically customizing visualizations based on user input. In contrast, dynamic client leverages Zarr to render the data directly using WebGL rather than through an intermediate layer. The Zarr format is ideal for direct rendering in the browser because the chunks of a Zarr dataset serve a similar purpose to the tiles of a web map. In addition, data can be chunked across non-spatial dimensions like time, which removes the requirement of generating individual tiles per time step. Lastly, Zarr is a cloud-optimized data format that allows for fast, parallel reading and writing from object storage.\nThe dynamic client approach leverages pyramids created with the ndpyramid package in order to performantly render data at multiple zoom levels. The approach loads Zarr data using the zarr-js JavaScript library and renders the fetched chunks via WebGL using the regl library. The open-source library called @carbonplan/maps provides a small set of React components for loading and rendering Zarr data using this approach and supports rendering traditional vector layers side-by-side using mapbox-gl-js.\nCarbonPlan has used this approach to quickly develop visualizations for climate science (some example visualizations).\n\n\nHistorically, building these visualizations has required creating a second, visualization-specific copy of the data. The primary factor being the gap between the low limit of request sizes that can be reasonably fetched by the browser (&lt;10 MB) and the conventional Zarr chunk sizes (~100 MB) used for analysis. With the introduction of the sharding extension to the Zarr V3 spec, both access patterns can now be accommodated by a single dataset.\n\n\n\nPyramids are used to improve rendering performance for most web mapping approaches. Pyramids contain successively lower resolution versions of the same dataset, which are commonly referred to as zoom levels or overviews. When viewing the entire dataset, the coarsest zoom level can be quickly fetched and rendered. Finer zoom levels are smoothly fetched and rendered as the user zooms in.\nThe dynamic client approach currently relies on pyramids in the Zarr stores, although we will explore relaxing this requirement in the future. ndpyramid is a small Python package providing utilities for generating N-dimensional Zarr pyramids using Xarray. Ndpyramid currently generates pyramids through either reprojection or coarsening. The prior requirement of web mercator pyramids has been removed from the dynamic client approach in the interest of directly visualizing analysis-ready datasets. The data can be reprojected in the client itself instead.",
    "crumbs": [
      "Approaches",
      "Dynamic Client"
    ]
  },
  {
    "objectID": "approaches/dynamic-client.html#background",
    "href": "approaches/dynamic-client.html#background",
    "title": "Dynamic Client",
    "section": "",
    "text": "Traditional approaches to rendering raster datasets in the browser involve the creation of tiles in a pixelated image format like PNG or JPEG. These tiles can be pre-generated or created by a tiling server on demand, as described in the tiling approach chapter. While the image tiles are fast to load and easy to render, tiling offers limited flexibility for dynamically customizing visualizations based on user input. In contrast, dynamic client leverages Zarr to render the data directly using WebGL rather than through an intermediate layer. The Zarr format is ideal for direct rendering in the browser because the chunks of a Zarr dataset serve a similar purpose to the tiles of a web map. In addition, data can be chunked across non-spatial dimensions like time, which removes the requirement of generating individual tiles per time step. Lastly, Zarr is a cloud-optimized data format that allows for fast, parallel reading and writing from object storage.\nThe dynamic client approach leverages pyramids created with the ndpyramid package in order to performantly render data at multiple zoom levels. The approach loads Zarr data using the zarr-js JavaScript library and renders the fetched chunks via WebGL using the regl library. The open-source library called @carbonplan/maps provides a small set of React components for loading and rendering Zarr data using this approach and supports rendering traditional vector layers side-by-side using mapbox-gl-js.\nCarbonPlan has used this approach to quickly develop visualizations for climate science (some example visualizations).\n\n\nHistorically, building these visualizations has required creating a second, visualization-specific copy of the data. The primary factor being the gap between the low limit of request sizes that can be reasonably fetched by the browser (&lt;10 MB) and the conventional Zarr chunk sizes (~100 MB) used for analysis. With the introduction of the sharding extension to the Zarr V3 spec, both access patterns can now be accommodated by a single dataset.\n\n\n\nPyramids are used to improve rendering performance for most web mapping approaches. Pyramids contain successively lower resolution versions of the same dataset, which are commonly referred to as zoom levels or overviews. When viewing the entire dataset, the coarsest zoom level can be quickly fetched and rendered. Finer zoom levels are smoothly fetched and rendered as the user zooms in.\nThe dynamic client approach currently relies on pyramids in the Zarr stores, although we will explore relaxing this requirement in the future. ndpyramid is a small Python package providing utilities for generating N-dimensional Zarr pyramids using Xarray. Ndpyramid currently generates pyramids through either reprojection or coarsening. The prior requirement of web mercator pyramids has been removed from the dynamic client approach in the interest of directly visualizing analysis-ready datasets. The data can be reprojected in the client itself instead.",
    "crumbs": [
      "Approaches",
      "Dynamic Client"
    ]
  },
  {
    "objectID": "approaches/dynamic-client.html#references",
    "href": "approaches/dynamic-client.html#references",
    "title": "Dynamic Client",
    "section": "References",
    "text": "References\nFreeman, J., K. Martin, and J. Hamman, 2021: A new toolkit for data-driven maps, https://carbonplan.org/blog/maps-library-release",
    "crumbs": [
      "Approaches",
      "Dynamic Client"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/e2e-results-zarr-version.html",
    "href": "approaches/dynamic-client/e2e-results-zarr-version.html",
    "title": "Benchmarking: Zarr Version",
    "section": "",
    "text": "import holoviews as hv\nimport hvplot\nimport hvplot.pandas  # noqa\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\n\n\n\n\n\n\nRead summary of all benchmarking results.\n\nsummary = pd.read_parquet(\"s3://carbonplan-benchmarks/benchmark-data/v0.2/summary.parq\")\n\nSubset the data to isolate the impact of Zarr version and chunk size.\n\ndf = summary[\n    (summary[\"projection\"] == 4326)\n    & (summary[\"pixels_per_tile\"] == 128)\n    & (summary[\"shard_size\"] == 0)\n    & (summary[\"region\"] == \"us-west-2\")\n]\n\nSet plot options.\n\ncmap = [\"#E1BE6A\", \"#40B0A6\"]\nplt_opts = {\"width\": 600, \"height\": 400}\n\nCreate a box plot showing how the rendering time depends on Zarr version and chunk size.\n\ndf.hvplot.box(\n    y=\"duration\",\n    by=[\"actual_chunk_size\", \"zarr_version\"],\n    c=\"zarr_version\",\n    cmap=cmap,\n    ylabel=\"Time to render (ms)\",\n    xlabel=\"Chunk size (MB); Zarr Version\",\n    legend=False,\n).opts(**plt_opts)\n\n\n\n\n\n  \n\n\n\n\nFit a multiple linear regression to the results. The results show that the chunk size strongly impacts the time to render. Datasets with larger chunk sizes take longer to render. The Zarr version does not have a noticeable impact on rendering time.\n\nmodel = smf.ols(\"duration ~ actual_chunk_size + C(zarr_version)\", data=df).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nduration\nR-squared:\n0.511\n\n\nModel:\nOLS\nAdj. R-squared:\n0.507\n\n\nMethod:\nLeast Squares\nF-statistic:\n132.4\n\n\nDate:\nSat, 02 Sep 2023\nProb (F-statistic):\n4.58e-40\n\n\nTime:\n19:45:37\nLog-Likelihood:\n-2050.1\n\n\nNo. Observations:\n256\nAIC:\n4106.\n\n\nDf Residuals:\n253\nBIC:\n4117.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2066.9020\n82.158\n25.158\n0.000\n1905.102\n2228.702\n\n\nC(zarr_version)[T.3]\n-17.5790\n91.439\n-0.192\n0.848\n-197.657\n162.499\n\n\nactual_chunk_size\n84.7372\n5.208\n16.269\n0.000\n74.480\n94.995\n\n\n\n\n\n\n\n\nOmnibus:\n37.985\nDurbin-Watson:\n1.955\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n52.057\n\n\nSkew:\n-0.953\nProb(JB):\n4.96e-12\n\n\nKurtosis:\n4.116\nCond. No.\n31.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nShow the rendering time at different zoom levels.\n\nplt_opts = {\"width\": 400, \"height\": 300}\n\nplts = []\n\nfor zoom_level in range(4):\n    df_level = df[df[\"zoom\"] == zoom_level]\n    plts.append(\n        df_level.hvplot.box(\n            y=\"duration\",\n            by=[\"actual_chunk_size\", \"zarr_version\"],\n            c=\"zarr_version\",\n            cmap=cmap,\n            ylabel=\"Time to render (ms)\",\n            xlabel=\"Chunk size (MB); Zarr version\",\n            legend=False,\n            title=f\"Zoom level {zoom_level}\",\n        ).opts(**plt_opts)\n    )\nhv.Layout(plts).cols(2)\n\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_drag' property; using the latest value\n  layout_plot = gridplot(\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_scroll' property; using the latest value\n  layout_plot = gridplot(\n\n\n\n\n\n\n  \n\n\n\n\nAdd a multiplicative interaction term with zoom level to the multiple linear regression. The results show that chunk size has a significant impact on rendering performance at higher zoom levels, with the most pronounced affect at zoom level 3.\n\nmodel = smf.ols(\"duration ~ actual_chunk_size * C(zoom)\", data=df).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nduration\nR-squared:\n0.919\n\n\nModel:\nOLS\nAdj. R-squared:\n0.917\n\n\nMethod:\nLeast Squares\nF-statistic:\n401.4\n\n\nDate:\nSat, 02 Sep 2023\nProb (F-statistic):\n2.29e-131\n\n\nTime:\n19:45:37\nLog-Likelihood:\n-1820.2\n\n\nNo. Observations:\n256\nAIC:\n3656.\n\n\nDf Residuals:\n248\nBIC:\n3685.\n\n\nDf Model:\n7\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2274.2785\n56.178\n40.484\n0.000\n2163.633\n2384.925\n\n\nC(zoom)[T.1.0]\n171.4040\n79.447\n2.157\n0.032\n14.927\n327.881\n\n\nC(zoom)[T.2.0]\n-595.6177\n79.447\n-7.497\n0.000\n-752.095\n-439.141\n\n\nC(zoom)[T.3.0]\n-440.4506\n79.447\n-5.544\n0.000\n-596.928\n-283.974\n\n\nactual_chunk_size\n-6.0398\n4.286\n-1.409\n0.160\n-14.482\n2.403\n\n\nactual_chunk_size:C(zoom)[T.1.0]\n71.2072\n6.062\n11.747\n0.000\n59.268\n83.147\n\n\nactual_chunk_size:C(zoom)[T.2.0]\n140.8571\n6.062\n23.236\n0.000\n128.918\n152.796\n\n\nactual_chunk_size:C(zoom)[T.3.0]\n151.0435\n6.062\n24.917\n0.000\n139.104\n162.983\n\n\n\n\n\n\n\n\nOmnibus:\n23.536\nDurbin-Watson:\n1.445\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n39.029\n\n\nSkew:\n0.545\nProb(JB):\n3.35e-09\n\n\nKurtosis:\n4.572\nCond. No.\n94.1\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking: Zarr Version"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/benchmarking-methodology.html",
    "href": "approaches/dynamic-client/benchmarking-methodology.html",
    "title": "Benchmarking Methodology",
    "section": "",
    "text": "The end-to-end benchmarks capture the user experience for various interactions. The suite of benchmarks included in this cookbook are designed to inform how the choice of Zarr versions and chunking schemes influence the user experience.\n\n\nWe used the publicly available NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) AWS public dataset for this project. For this demonstration, we used two years of the the daily maximum near-surface air temperature (tasmax) variable from the ACCESS-CM2 climate model.\nWe first transformed the NetCDF files hosted on S3 to Zarr stores, with the full notebook available in the benchmark-maps repository.\nNext, we used ndpyramid to generate pyramids for the Zarr store. The notebook for generating pyramids is available in the benchmark-maps repository. We created pyramids containing four zoom levels using the pyramid_reproject function in ndpyramid. We generated pyramids using multiple data configurations, including 128 and 256 pixels per tile in the spatial dimensions and 1MB, 5MB, 10MB, and 25MB target chunk sizes. The chunk size along the time dimension was the largest number of time slices that would evenly divide the time dimension and produce an uncompressed chunk that did not exceed the target size. For most use-cases, the time dimension would not need to be evenly divisible by the chunk size; this was only necessary for the comparison with V3 sharded datasets. The data were encoded as float32 while the time coordinate was encoded as int32 using level 1 gzip compression.\nWe used the experimental zarrita library to convert the pyramids to Zarr V3 data for performance testing. The data leveraged the same encoding as the Zarr V2 pyramids, with the addition of a sharding codec. The target shard sizes were 25MB, 50MB, and 100MB and the chunk size within each shard was equivalent to the V2 chunking scheme.\n\n\n\nCarbonPlan’s benchmark-maps repository leverages Playwright for the end-to-end performance benchmarks. By default, the benchmarks are run on https://prototype-maps.vercel.app/ although the url is configurable. The dynamic client prototype library shows this domain after selecting a dataset and Zarr version.\nThe benchmarking script takes the following steps:\n\nLaunch chromium browser\nCreate a new page\nStart chromium tracing\nNavigate to web mapping application\nSelect Dataset in the dropdown\nWait 5 seconds for the page the render\nZoom in a defined number of times, waiting 5 seconds after each action\nWrite out metadata about each run and the trace record\n\n\n\n\nThe benchmark-maps repository can be used to run the benchmarking suite. The first step is to clone the repository:\ngit clone https://github.com/carbonplan/benchmark-maps.git\nThe next step is to create an environment for running the benchmarks. We recommend mamba for managing the environment. You will also need to install the required dependencies for playwright:\nmamba env create --file binder/environment.yml\nmamba activate benchmark-maps\nplaywright install\nOnce the environment is set up, you can run the benchmarks by running the following command:\ncarbonplan_benchmarks --dataset pyramids-v2-3857-True-128-1-0-0-f4-0-0-0-gzipL1-100 --action zoom_in --zoom-level 4\nIn addition, main.sh in the benchmark-maps repository is a script for running multiple iterations of the benchmarks on multiple datasets and Zarr versions.\n\n\n\nEach benchmark yields a metadata file and trace record. The carbonplan_benchmarks Python package provides utilities for analyzing and visualizing these outputs. For each interaction (e.g., loading the page, zooming in), we extracted information about the requests (e.g., duration, URL, encoded data length), frames (e.g., duration, status), and calculated the amount of time before rendering was complete. Note that these metrics do not consider the fact that the time to render the first part of the data on the page strongly influences the user experience and would be much faster than the time to render the entire page.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking Methodology"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/benchmarking-methodology.html#end-to-end-benchmarks",
    "href": "approaches/dynamic-client/benchmarking-methodology.html#end-to-end-benchmarks",
    "title": "Benchmarking Methodology",
    "section": "",
    "text": "The end-to-end benchmarks capture the user experience for various interactions. The suite of benchmarks included in this cookbook are designed to inform how the choice of Zarr versions and chunking schemes influence the user experience.\n\n\nWe used the publicly available NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) AWS public dataset for this project. For this demonstration, we used two years of the the daily maximum near-surface air temperature (tasmax) variable from the ACCESS-CM2 climate model.\nWe first transformed the NetCDF files hosted on S3 to Zarr stores, with the full notebook available in the benchmark-maps repository.\nNext, we used ndpyramid to generate pyramids for the Zarr store. The notebook for generating pyramids is available in the benchmark-maps repository. We created pyramids containing four zoom levels using the pyramid_reproject function in ndpyramid. We generated pyramids using multiple data configurations, including 128 and 256 pixels per tile in the spatial dimensions and 1MB, 5MB, 10MB, and 25MB target chunk sizes. The chunk size along the time dimension was the largest number of time slices that would evenly divide the time dimension and produce an uncompressed chunk that did not exceed the target size. For most use-cases, the time dimension would not need to be evenly divisible by the chunk size; this was only necessary for the comparison with V3 sharded datasets. The data were encoded as float32 while the time coordinate was encoded as int32 using level 1 gzip compression.\nWe used the experimental zarrita library to convert the pyramids to Zarr V3 data for performance testing. The data leveraged the same encoding as the Zarr V2 pyramids, with the addition of a sharding codec. The target shard sizes were 25MB, 50MB, and 100MB and the chunk size within each shard was equivalent to the V2 chunking scheme.\n\n\n\nCarbonPlan’s benchmark-maps repository leverages Playwright for the end-to-end performance benchmarks. By default, the benchmarks are run on https://prototype-maps.vercel.app/ although the url is configurable. The dynamic client prototype library shows this domain after selecting a dataset and Zarr version.\nThe benchmarking script takes the following steps:\n\nLaunch chromium browser\nCreate a new page\nStart chromium tracing\nNavigate to web mapping application\nSelect Dataset in the dropdown\nWait 5 seconds for the page the render\nZoom in a defined number of times, waiting 5 seconds after each action\nWrite out metadata about each run and the trace record\n\n\n\n\nThe benchmark-maps repository can be used to run the benchmarking suite. The first step is to clone the repository:\ngit clone https://github.com/carbonplan/benchmark-maps.git\nThe next step is to create an environment for running the benchmarks. We recommend mamba for managing the environment. You will also need to install the required dependencies for playwright:\nmamba env create --file binder/environment.yml\nmamba activate benchmark-maps\nplaywright install\nOnce the environment is set up, you can run the benchmarks by running the following command:\ncarbonplan_benchmarks --dataset pyramids-v2-3857-True-128-1-0-0-f4-0-0-0-gzipL1-100 --action zoom_in --zoom-level 4\nIn addition, main.sh in the benchmark-maps repository is a script for running multiple iterations of the benchmarks on multiple datasets and Zarr versions.\n\n\n\nEach benchmark yields a metadata file and trace record. The carbonplan_benchmarks Python package provides utilities for analyzing and visualizing these outputs. For each interaction (e.g., loading the page, zooming in), we extracted information about the requests (e.g., duration, URL, encoded data length), frames (e.g., duration, status), and calculated the amount of time before rendering was complete. Note that these metrics do not consider the fact that the time to render the first part of the data on the page strongly influences the user experience and would be much faster than the time to render the entire page.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking Methodology"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/e2e-results-zarr-version-shards.html",
    "href": "approaches/dynamic-client/e2e-results-zarr-version-shards.html",
    "title": "Benchmarking: Sharding Extension",
    "section": "",
    "text": "import holoviews as hv\nimport hvplot\nimport hvplot.pandas  # noqa\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\n\n\n\n\n\n\nRead summary of all benchmarking results.\n\nsummary = pd.read_parquet(\"s3://carbonplan-benchmarks/benchmark-data/v0.2/summary.parq\")\n\nSubset the data to isolate the impact of Zarr version when using the sharding extension for V3 data and chunk size.\n\ndf = summary[\n    (summary[\"projection\"] == 3857)\n    & (summary[\"pixels_per_tile\"] == 128)\n    & ((summary[\"zarr_version\"] == 2) | (summary[\"shard_size\"] == 100))\n    & (summary[\"region\"] == \"us-west-2\")\n]\n\n\ncmap = [\"#E1BE6A\", \"#40B0A6\"]\nplt_opts = {\"width\": 600, \"height\": 400}\n\nCreate a box plot showing how the rendering time depends on Zarr version when using the sharding extension for V3 data and chunk size.\n\ndf.hvplot.box(\n    y=\"duration\",\n    by=[\"actual_chunk_size\", \"zarr_version\"],\n    c=\"zarr_version\",\n    cmap=cmap,\n    ylabel=\"Time to render (ms)\",\n    xlabel=\"Chunk size (MB); Zarr Version\",\n    legend=False,\n).opts(**plt_opts)\n\n\n\n\n\n  \n\n\n\n\nFit a multiple linear regression to the results. The results show that rendering Zarr V3 data with the sharding extension is slower than rendering Zarr V2 data, but most of the variance in rendering time is unexplained by that variable alone.\n\nmodel = smf.ols(\"duration ~ actual_chunk_size + C(zarr_version)\", data=df).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nduration\nR-squared:\n0.320\n\n\nModel:\nOLS\nAdj. R-squared:\n0.315\n\n\nMethod:\nLeast Squares\nF-statistic:\n59.65\n\n\nDate:\nTue, 29 Aug 2023\nProb (F-statistic):\n5.99e-22\n\n\nTime:\n20:31:22\nLog-Likelihood:\n-1988.5\n\n\nNo. Observations:\n256\nAIC:\n3983.\n\n\nDf Residuals:\n253\nBIC:\n3994.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1914.6479\n64.596\n29.640\n0.000\n1787.434\n2041.862\n\n\nC(zarr_version)[T.3]\n268.8945\n71.893\n3.740\n0.000\n127.310\n410.479\n\n\nactual_chunk_size\n42.0231\n4.095\n10.262\n0.000\n33.958\n50.088\n\n\n\n\n\n\n\n\nOmnibus:\n7.661\nDurbin-Watson:\n2.198\n\n\nProb(Omnibus):\n0.022\nJarque-Bera (JB):\n6.705\n\n\nSkew:\n0.323\nProb(JB):\n0.0350\n\n\nKurtosis:\n2.539\nCond. No.\n31.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nShow the rendering time at different zoom levels.\n\nplt_opts = {\"width\": 400, \"height\": 300}\n\nplts = []\n\nfor zoom_level in range(4):\n    df_level = df[df[\"zoom\"] == zoom_level]\n    plts.append(\n        df_level.hvplot.box(\n            y=\"duration\",\n            by=[\"actual_chunk_size\", \"zarr_version\"],\n            c=\"zarr_version\",\n            cmap=cmap,\n            ylabel=\"Time to render (ms)\",\n            xlabel=\"Chunk size (MB); Zarr version\",\n            legend=False,\n            title=f\"Zoom level {zoom_level}\",\n        ).opts(**plt_opts)\n    )\nhv.Layout(plts).cols(2)\n\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_drag' property; using the latest value\n  layout_plot = gridplot(\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_scroll' property; using the latest value\n  layout_plot = gridplot(\n\n\n\n\n\n\n  \n\n\n\n\nAdd a multiplicative interaction term with zoom level to the multiple linear regression. The results show that chunk size has a significant impact on rendering performance at higher zoom levels, with the most pronounced affect at zoom level 3. Zarr V3 data with sharding renders faster than Zarr V2 data at zoom level 0 but slower at higher zoom levels.\n\nmodel = smf.ols(\n    \"duration ~ actual_chunk_size * C(zoom) + C(zarr_version) * C(zoom) + actual_chunk_size * C(zarr_version)\",  # noqa\n    data=df,\n).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nduration\nR-squared:\n0.948\n\n\nModel:\nOLS\nAdj. R-squared:\n0.945\n\n\nMethod:\nLeast Squares\nF-statistic:\n369.4\n\n\nDate:\nTue, 29 Aug 2023\nProb (F-statistic):\n1.74e-148\n\n\nTime:\n20:31:23\nLog-Likelihood:\n-1659.4\n\n\nNo. Observations:\n256\nAIC:\n3345.\n\n\nDf Residuals:\n243\nBIC:\n3391.\n\n\nDf Model:\n12\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2225.1492\n38.148\n58.329\n0.000\n2150.006\n2300.292\n\n\nC(zoom)[T.1.0]\n210.8908\n51.552\n4.091\n0.000\n109.345\n312.437\n\n\nC(zoom)[T.2.0]\n-578.2420\n51.552\n-11.217\n0.000\n-679.788\n-476.696\n\n\nC(zoom)[T.3.0]\n-1050.6001\n51.552\n-20.379\n0.000\n-1152.146\n-949.054\n\n\nC(zarr_version)[T.3]\n-34.4019\n46.388\n-0.742\n0.459\n-125.775\n56.971\n\n\nC(zarr_version)[T.3]:C(zoom)[T.1.0]\n612.5118\n57.376\n10.675\n0.000\n499.494\n725.529\n\n\nC(zarr_version)[T.3]:C(zoom)[T.2.0]\n412.6057\n57.376\n7.191\n0.000\n299.588\n525.623\n\n\nC(zarr_version)[T.3]:C(zoom)[T.3.0]\n539.9608\n57.376\n9.411\n0.000\n426.943\n652.978\n\n\nactual_chunk_size\n0.5489\n2.584\n0.212\n0.832\n-4.540\n5.638\n\n\nactual_chunk_size:C(zoom)[T.1.0]\n58.0800\n3.268\n17.771\n0.000\n51.642\n64.518\n\n\nactual_chunk_size:C(zoom)[T.2.0]\n63.2566\n3.268\n19.355\n0.000\n56.819\n69.694\n\n\nactual_chunk_size:C(zoom)[T.3.0]\n62.6391\n3.268\n19.166\n0.000\n56.202\n69.077\n\n\nactual_chunk_size:C(zarr_version)[T.3]\n-9.0395\n2.311\n-3.912\n0.000\n-13.592\n-4.487\n\n\n\n\n\n\n\n\nOmnibus:\n0.970\nDurbin-Watson:\n1.815\n\n\nProb(Omnibus):\n0.616\nJarque-Bera (JB):\n0.681\n\n\nSkew:\n-0.046\nProb(JB):\n0.712\n\n\nKurtosis:\n3.235\nCond. No.\n168.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking: Sharding Extension"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/e2e-results-aws-region.html",
    "href": "approaches/dynamic-client/e2e-results-aws-region.html",
    "title": "Benchmarking: AWS Region",
    "section": "",
    "text": "import hvplot\nimport hvplot.pandas  # noqa\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\n\n\n\n\n\n\nRead summary of all benchmarking results.\n\nsummary = pd.read_parquet(\"s3://carbonplan-benchmarks/benchmark-data/v0.2/summary.parq\")\n\nSubset the data to isolate the impact of location and chunk size.\n\ndf = summary[\n    (summary[\"projection\"] == 3857)\n    & (summary[\"pixels_per_tile\"] == 128)\n    & (summary[\"shard_size\"] == 0)\n]\n\nSet plot options.\n\ncmap = [\"#FFC20A\", \"#0C7BDC\"]\nplt_opts = {\"width\": 600, \"height\": 400}\n\nCreate a box plot showing how the rendering time depends on the AWS region and chunk size.\n\ndf.hvplot.box(\n    y=\"duration\",\n    by=[\"actual_chunk_size\", \"region\"],\n    c=\"region\",\n    cmap=cmap,\n    ylabel=\"Time to render (ms)\",\n    xlabel=\"Chunk size (MB); AWS region\",\n    legend=False,\n).opts(**plt_opts)\n\n\n\n\n\n  \n\n\n\n\nFit a multiple linear regression to the results. The results show that the chunk size strongly impacts the time to render the data. Datasets with larger chunk sizes take longer to render. The AWS region does not have a noticeable impact on rendering time.\n\nmodel = smf.ols(\"duration ~ actual_chunk_size + C(region)\", data=df).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nduration\nR-squared:\n0.446\n\n\nModel:\nOLS\nAdj. R-squared:\n0.444\n\n\nMethod:\nLeast Squares\nF-statistic:\n205.1\n\n\nDate:\nTue, 29 Aug 2023\nProb (F-statistic):\n4.58e-66\n\n\nTime:\n20:28:30\nLog-Likelihood:\n-3916.0\n\n\nNo. Observations:\n512\nAIC:\n7838.\n\n\nDf Residuals:\n509\nBIC:\n7851.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1859.2163\n40.422\n45.995\n0.000\n1779.801\n1938.631\n\n\nC(region)[T.us-west-2]\n-53.6344\n44.989\n-1.192\n0.234\n-142.021\n34.752\n\n\nactual_chunk_size\n51.8170\n2.563\n20.221\n0.000\n46.782\n56.852\n\n\n\n\n\n\n\n\nOmnibus:\n22.416\nDurbin-Watson:\n1.979\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n12.956\n\n\nSkew:\n0.227\nProb(JB):\n0.00154\n\n\nKurtosis:\n2.367\nCond. No.\n31.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking: AWS Region"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/prototype-deployment.html",
    "href": "approaches/dynamic-client/prototype-deployment.html",
    "title": "Prototype Deployment",
    "section": "",
    "text": "We started the prototype-maps project to support visualizing Zarr data using multiple approaches and data configurations in a consolidated application. The project is currently deployed at http://prototype-maps.demo.carbonplan.org/. The current version of the dynamic client page includes the option to select from several different datasets. Datasets are available in either Zarr V2 or Zarr V3 format, with different projections (equirectangular or mercator), chunking schemes (~1MB/chunk, ~5MB/chunk, ~10MB/chunk, or ~25MB/chunk), and sharding schemes (none, ~50MB/shard, ~100MB/shard). The time slider allows scrolling through the two years of daily data included in this demonstration. Options for adjusting the color mapping and visualizing time series are included in drop-down windows, with the spatial selector allowing custom selection of the data window averaged for the time series.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Prototype Deployment"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/prototype-deployment.html#prototype-maps",
    "href": "approaches/dynamic-client/prototype-deployment.html#prototype-maps",
    "title": "Prototype Deployment",
    "section": "",
    "text": "We started the prototype-maps project to support visualizing Zarr data using multiple approaches and data configurations in a consolidated application. The project is currently deployed at http://prototype-maps.demo.carbonplan.org/. The current version of the dynamic client page includes the option to select from several different datasets. Datasets are available in either Zarr V2 or Zarr V3 format, with different projections (equirectangular or mercator), chunking schemes (~1MB/chunk, ~5MB/chunk, ~10MB/chunk, or ~25MB/chunk), and sharding schemes (none, ~50MB/shard, ~100MB/shard). The time slider allows scrolling through the two years of daily data included in this demonstration. Options for adjusting the color mapping and visualizing time series are included in drop-down windows, with the spatial selector allowing custom selection of the data window averaged for the time series.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Prototype Deployment"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/prototype-deployment.html#prototype-maps-technology-stack",
    "href": "approaches/dynamic-client/prototype-deployment.html#prototype-maps-technology-stack",
    "title": "Prototype Deployment",
    "section": "prototype-maps: Technology stack",
    "text": "prototype-maps: Technology stack\nThe prototype deployment was built using @carbonplan/maps with different configurations of projection and Zarr version, for which support was added in 3.1.0. Upstream, the @carbonplan/maps library depends on zarr-js@3.3.0, for support reading Zarr in v3 and sharded formats as well as mapbox-gl@1.13.1 for the mapping context in which the Zarr data are rendered.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Prototype Deployment"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/e2e-results-projection.html",
    "href": "approaches/dynamic-client/e2e-results-projection.html",
    "title": "Benchmarking: Projection",
    "section": "",
    "text": "import holoviews as hv\nimport hvplot\nimport hvplot.pandas  # noqa\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\n\n\n\n\n\n\nRead summary of all benchmarking results.\n\nsummary = pd.read_parquet(\"s3://carbonplan-benchmarks/benchmark-data/v0.2/summary.parq\")\n\nSubset the data to isolate the impact of projection and chunk size.\n\ndf = summary[(summary[\"pixels_per_tile\"] == 128) & (summary[\"region\"] == \"us-west-2\")]\n\nSet plot options.\n\ncmap = [\"#E66100\", \"#5D3A9B\"]\nplt_opts = {\"width\": 600, \"height\": 400}\n\nCreate a box plot showing how the rendering time depends on Zarr version and chunk size.\n\ndf.hvplot.box(\n    y=\"duration\",\n    by=[\"actual_chunk_size\", \"projection\"],\n    c=\"projection\",\n    cmap=cmap,\n    ylabel=\"Time to render (ms)\",\n    xlabel=\"Chunk size (MB); EPSG number\",\n    legend=False,\n).opts(**plt_opts)\n\n\n\n\n\n  \n\n\n\n\nFit a multiple linear regression to the results. The results show that the projection, along with the chunk size, strongly impacts the rendering time, with web mercator pyramids rendering faster than equidistant cylindrical pyramids.\n\nmodel = smf.ols(\"duration ~ actual_chunk_size + C(projection)\", data=df).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nduration\nR-squared:\n0.431\n\n\nModel:\nOLS\nAdj. R-squared:\n0.430\n\n\nMethod:\nLeast Squares\nF-statistic:\n387.2\n\n\nDate:\nTue, 29 Aug 2023\nProb (F-statistic):\n7.16e-126\n\n\nTime:\n20:29:50\nLog-Likelihood:\n-8152.5\n\n\nNo. Observations:\n1024\nAIC:\n1.631e+04\n\n\nDf Residuals:\n1021\nBIC:\n1.633e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1864.6662\n39.029\n47.776\n0.000\n1788.080\n1941.252\n\n\nC(projection)[T.4326]\n580.6684\n43.438\n13.368\n0.000\n495.430\n665.906\n\n\nactual_chunk_size\n60.3908\n2.474\n24.408\n0.000\n55.536\n65.246\n\n\n\n\n\n\n\n\nOmnibus:\n7.834\nDurbin-Watson:\n1.830\n\n\nProb(Omnibus):\n0.020\nJarque-Bera (JB):\n7.973\n\n\nSkew:\n-0.210\nProb(JB):\n0.0186\n\n\nKurtosis:\n2.897\nCond. No.\n31.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nShow the rendering time at different zoom levels.\n\nplt_opts = {\"width\": 400, \"height\": 300}\n\nplts = []\n\nfor zoom_level in range(4):\n    df_level = df[df[\"zoom\"] == zoom_level]\n    plts.append(\n        df_level.hvplot.box(\n            y=\"duration\",\n            by=[\"actual_chunk_size\", \"projection\"],\n            c=\"projection\",\n            cmap=cmap,\n            ylabel=\"Time to render (ms)\",\n            xlabel=\"Chunk size (MB); EPSG number\",\n            legend=False,\n            title=f\"Zoom level {zoom_level}\",\n        ).opts(**plt_opts)\n    )\nhv.Layout(plts).cols(2)\n\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_drag' property; using the latest value\n  layout_plot = gridplot(\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_scroll' property; using the latest value\n  layout_plot = gridplot(\n\n\n\n\n\n\n  \n\n\n\n\nAdd a multiplicative interaction term with zoom level to the multiple linear regression. The results show that projection has a significant impact on rendering performance at higher zoom levels, with the most pronounced affect at zoom level 3. Larger chunk size increases the amount of time for rendering equidistant cylindrical pyramids relative to web mercator pyramids.\n\nmodel = smf.ols(\n    \"duration ~ actual_chunk_size * C(projection) + C(projection) * C(zoom)\", data=df\n).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nduration\nR-squared:\n0.729\n\n\nModel:\nOLS\nAdj. R-squared:\n0.726\n\n\nMethod:\nLeast Squares\nF-statistic:\n302.5\n\n\nDate:\nTue, 29 Aug 2023\nProb (F-statistic):\n5.84e-280\n\n\nTime:\n20:29:51\nLog-Likelihood:\n-7773.7\n\n\nNo. Observations:\n1024\nAIC:\n1.557e+04\n\n\nDf Residuals:\n1014\nBIC:\n1.562e+04\n\n\nDf Model:\n9\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1760.2638\n48.690\n36.152\n0.000\n1664.719\n1855.809\n\n\nC(projection)[T.4326]\n-311.9528\n68.858\n-4.530\n0.000\n-447.074\n-176.832\n\n\nC(zoom)[T.1.0]\n1043.0435\n60.224\n17.319\n0.000\n924.866\n1161.221\n\n\nC(zoom)[T.2.0]\n239.5532\n60.224\n3.978\n0.000\n121.376\n357.731\n\n\nC(zoom)[T.3.0]\n-186.3396\n60.224\n-3.094\n0.002\n-304.517\n-68.162\n\n\nC(projection)[T.4326]:C(zoom)[T.1.0]\n42.7015\n85.169\n0.501\n0.616\n-124.427\n209.830\n\n\nC(projection)[T.4326]:C(zoom)[T.2.0]\n741.8614\n85.169\n8.710\n0.000\n574.733\n908.990\n\n\nC(projection)[T.4326]:C(zoom)[T.3.0]\n1428.6270\n85.169\n16.774\n0.000\n1261.499\n1595.755\n\n\nactual_chunk_size\n42.9576\n2.426\n17.710\n0.000\n38.198\n47.717\n\n\nactual_chunk_size:C(projection)[T.4326]\n34.8665\n3.430\n10.164\n0.000\n28.135\n41.598\n\n\n\n\n\n\n\n\nOmnibus:\n78.010\nDurbin-Watson:\n1.395\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n95.056\n\n\nSkew:\n-0.699\nProb(JB):\n2.29e-21\n\n\nKurtosis:\n3.525\nCond. No.\n151.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking: Projection"
    ]
  },
  {
    "objectID": "approaches/tiling/04-number-of-spatial-chunks.html",
    "href": "approaches/tiling/04-number-of-spatial-chunks.html",
    "title": "Benchmarks 4: Spatial Chunk Variation",
    "section": "",
    "text": "In the previous notebook, we saw how chunk size impacts performance. However, using a small chunk size will result in more chunks. In this notebook, we explore how the number of chunks spatially can impact performance, especially at low zoom levels.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 4: Spatial Chunk Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/04-number-of-spatial-chunks.html#explanation",
    "href": "approaches/tiling/04-number-of-spatial-chunks.html#explanation",
    "title": "Benchmarks 4: Spatial Chunk Variation",
    "section": "",
    "text": "In the previous notebook, we saw how chunk size impacts performance. However, using a small chunk size will result in more chunks. In this notebook, we explore how the number of chunks spatially can impact performance, especially at low zoom levels.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 4: Spatial Chunk Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/04-number-of-spatial-chunks.html#dataset-generation",
    "href": "approaches/tiling/04-number-of-spatial-chunks.html#dataset-generation",
    "title": "Benchmarks 4: Spatial Chunk Variation",
    "section": "Dataset Generation",
    "text": "Dataset Generation\nWe compared the performance of tiling artificially generated Zarr data with constant chunk size and increased the spatial resolution, so a varied number of chunks is required for spatial coverage.\nThe code to produce the zarr stores are in the tile-benchmarking repo: 01-generate-datasets/generate-fake-data-with-chunks.ipynb.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 4: Spatial Chunk Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/04-number-of-spatial-chunks.html#tests",
    "href": "approaches/tiling/04-number-of-spatial-chunks.html#tests",
    "title": "Benchmarks 4: Spatial Chunk Variation",
    "section": "Tests",
    "text": "Tests\nTests were run via the tile-benchmarking/02-run-tests/04-number-of-spatial-chunks.ipynb notebook.\n\nimport pandas as pd\nimport warnings\nimport holoviews as hv\n\npd.options.plotting.backend = \"holoviews\"\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\n\n\n\n\n\n\n\ngit_url_path = \"https://raw.githubusercontent.com/developmentseed/tile-benchmarking/main/02-run-tests/results-csvs/\"\ndf = pd.read_csv(f\"{git_url_path}/04-number-of-spatial-chunks-results.csv\")\n\n\nzooms = range(6)\n\nplt_opts = {\"width\": 400, \"height\": 300}\n\nplts = []\n\nfor zoom_level in zooms:\n    df_level = df[df[\"zoom\"] == zoom_level]\n    plts.append(\n        df_level.hvplot.box(\n            y=\"time\",\n            by=[\"number_of_spatial_chunks\"],\n            c=\"number_of_spatial_chunks\",\n            cmap=\"Plasma_r\",\n            ylabel=\"Time to render (ms)\",\n            xlabel=\"Number of spatial chunks\",\n            legend=False,\n            title=f\"Zoom level {zoom_level}\",\n        ).opts(**plt_opts)\n    )\nhv.Layout(plts).cols(2)",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 4: Spatial Chunk Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/04-number-of-spatial-chunks.html#interpretation-of-the-results",
    "href": "approaches/tiling/04-number-of-spatial-chunks.html#interpretation-of-the-results",
    "title": "Benchmarks 4: Spatial Chunk Variation",
    "section": "Interpretation of the Results",
    "text": "Interpretation of the Results\nHaving a greater number of spatial chunks degrades performance at low zoom levels as seen above most notably for zooms 0 and 1. At high zoom levels, since fewer chunks need to be loaded, there is no difference in performance.\nWe can solve the problem of slow performance at low zoom levels with pyramids, or multiscale, datasets, as demonstrated in Benchmarks: Pyramids.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 4: Spatial Chunk Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/02-cog-zarr-kerchunk.html",
    "href": "approaches/tiling/02-cog-zarr-kerchunk.html",
    "title": "Benchmarks 2: Tile Generation Benchmarks across Data Formats",
    "section": "",
    "text": "This page shared results from benchmarking the performance of tiling CMIP6 data stored as COG, NetCDF and Zarr.\nThe intention is to understand the performance tradeoff between these data formats. These results should not be considered conclusive as additional library and caching improvements may be made in the future.\nIn order to tile the NetCDF, we use a kerchunk reference file. You are able to use the ZarrReader with NetCDF files without a kerchunk reference file, however you cannot read more than file at once which makes it incomparable with the pgSTAC+COG and Zarr methods.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 2: Tile Generation Benchmarks across Data Formats"
    ]
  },
  {
    "objectID": "approaches/tiling/02-cog-zarr-kerchunk.html#explanation",
    "href": "approaches/tiling/02-cog-zarr-kerchunk.html#explanation",
    "title": "Benchmarks 2: Tile Generation Benchmarks across Data Formats",
    "section": "",
    "text": "This page shared results from benchmarking the performance of tiling CMIP6 data stored as COG, NetCDF and Zarr.\nThe intention is to understand the performance tradeoff between these data formats. These results should not be considered conclusive as additional library and caching improvements may be made in the future.\nIn order to tile the NetCDF, we use a kerchunk reference file. You are able to use the ZarrReader with NetCDF files without a kerchunk reference file, however you cannot read more than file at once which makes it incomparable with the pgSTAC+COG and Zarr methods.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 2: Tile Generation Benchmarks across Data Formats"
    ]
  },
  {
    "objectID": "approaches/tiling/02-cog-zarr-kerchunk.html#dataset-generation",
    "href": "approaches/tiling/02-cog-zarr-kerchunk.html#dataset-generation",
    "title": "Benchmarks 2: Tile Generation Benchmarks across Data Formats",
    "section": "Dataset Generation",
    "text": "Dataset Generation\nThe test datasets produced and benchmarked are:\n\nCloud-Optimized GeoTiffs (COGs): Tiles are produced using the publicly available CMIP6 COGs in the s3://nex-gddp-cmip6-cog/ bucket. The metadata generation is documented in 01-cmip6-cog-tile-server-benchmarks.ipynb.\nkerchunk + netCDF: Tiles are produced using a kerchunk reference file generated for the NetCDF files stored in the s3://nex-gddp-cmip6 bucket. The code to produce the kerchunk reference is in the tile-benchmarking repo: 01-generate-datasets/generate-cmip6-kerchunk.ipynb.\nZarr: Tiles are produced using a zarr store with the same chunking configuration and the underlying NetCDFs. The code to produce the zarr store is in the tile-benchmarking repo: 01-generate-datasets/generate-cmip6-zarr.ipynb.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 2: Tile Generation Benchmarks across Data Formats"
    ]
  },
  {
    "objectID": "approaches/tiling/02-cog-zarr-kerchunk.html#tests",
    "href": "approaches/tiling/02-cog-zarr-kerchunk.html#tests",
    "title": "Benchmarks 2: Tile Generation Benchmarks across Data Formats",
    "section": "Tests",
    "text": "Tests\nTests were run via the tile-benchmarking/02-run-tests/02-cog-kerchunk-zarr.ipynb notebook.\n\nimport pandas as pd\nimport warnings\nimport holoviews as hv\n\npd.options.plotting.backend = \"holoviews\"\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\n\n\n\n\n\n\n\ngit_url_path = \"https://raw.githubusercontent.com/developmentseed/tile-benchmarking/main/02-run-tests/results-csvs/\"\ndf = pd.read_csv(f\"{git_url_path}/02-cog-kerchunk-zarr-results.csv\")\n\n\nzooms = range(6)\ncmap = [\"#E1BE6A\", \"#40B0A6\", \"#0C7BDC\"]\nplt_opts = {\"width\": 300, \"height\": 250}\n\nplts = []\n\nfor zoom_level in zooms:\n    df_level = df[df[\"zoom\"] == zoom_level]\n    plts.append(\n        df_level.hvplot.box(\n            y=\"time\",\n            by=[\"data_format\"],\n            c=\"data_format\",\n            cmap=cmap,\n            ylabel=\"Time to render (ms)\",\n            xlabel=\"Data Format\",\n            legend=False,\n            title=f\"Zoom level {zoom_level}\",\n        ).opts(**plt_opts)\n    )\n\nhv.Layout(plts).cols(2)",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 2: Tile Generation Benchmarks across Data Formats"
    ]
  },
  {
    "objectID": "approaches/tiling/02-cog-zarr-kerchunk.html#interpretation-of-the-results",
    "href": "approaches/tiling/02-cog-zarr-kerchunk.html#interpretation-of-the-results",
    "title": "Benchmarks 2: Tile Generation Benchmarks across Data Formats",
    "section": "Interpretation of the Results",
    "text": "Interpretation of the Results\n\nTiling COGs performs better than tiling Zarr or the kerchunk reference, at all zoom levels.\nThe performance of the kerchunk reference is better than the Zarr store. It is important to consider this is because the NetCDF files’ chunks are the same. Even though 365 time steps (days) are stored in each NetCDF file, it is chunked by day.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 2: Tile Generation Benchmarks across Data Formats"
    ]
  },
  {
    "objectID": "approaches/tiling/benchmarking-methodology.html",
    "href": "approaches/tiling/benchmarking-methodology.html",
    "title": "Benchmarking Methodology",
    "section": "",
    "text": "Benchmarks are provided for a deployed API (end-to-end tests, described below) as well as for the application code.\nScripts for generating the data and running the benchmarks can be found in https://github.com/developmentseed/tile-benchmarking.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarking Methodology"
    ]
  },
  {
    "objectID": "approaches/tiling/benchmarking-methodology.html#code-benchmarks",
    "href": "approaches/tiling/benchmarking-methodology.html#code-benchmarks",
    "title": "Benchmarking Methodology",
    "section": "Code Benchmarks",
    "text": "Code Benchmarks\nWhile the end-to-end benchmarks described below establish tiling performance as experienced by an end user, the introduction of the network introduce variables which are difficult to control, specifically the many varieties and locations of a client and server.\nApplication code benchmarks provide a clear understanding of performance for the code alone due to differences across datasets.\n\nCode Benchmarks: Datasets\nReproducibility is important to the integrity of this project and its results. We used a publicly available dataset and attempt to make the steps as fully documented and reproducible as possible. For code profiling, we focused the NASA Earth Exchange Global Daily Downscaled Projections (NEX-GDDP-CMIP6) AWS public dataset for this project.\nThere are 2 datasets listed on AWS from this project. One (1) is an archive of NetCDF files from about 35 different climate models, each supplying historical and predicted values for up to 9 environment variables, daily, from 1950 to 2100. To minimize preprocessing, test datasets were generated for the first 2 years of historical data, for 1 model and for 1 variable. These variables or dataset could easily be modified or swapped, but we expect the relative performance using different datasets to be the same.\nIn addition to the NetCDF data, there is an archive of COGs generated from the corresponding NetCDFs to support visualization via dynamic tiling using COGs. COGs are only available for 2 models, so for intercomparison of the tiling approach between COGs and Zarr, one of those models (GISS-E2-1-G) is used to generate Zarr stores.\nBenchmarks were generated for multiple copies of the CMIP6 daily data and some synthetic data to understand the performance for different data pre-processing options. These copies were differentiated by:\n\ndata file format (COG, NetCDF and Zarr)\ndifferent chunking configurations\npyramids\n\nNote: At this time, a different model is used for the dynamic client benchmarks (ACCESS-CM2), but we believe the test results and corresponding recommendations would not change for different CMIP6 models.\nBenchmarks were run on the VEDA JupyterHub. The VEDA documentation details how to request access: https://nasa-impact.github.io/veda-docs/services/jupyterhub.html. We chose not to make the database or S3 bucket nasa-eodc-data-store fully public. You must be logged into the VEDA JupyterHub to run those benchmarks or reproduce the test datasets.\n\n\nCode Benchmarks: Approach\nWe include performance results of code for tiling both COGs and Zarr to help data providers decide which format better suits their overall needs. To make these results comparable, we assume the following process for creating image tiles:\n\nStart with a known collection and query parameters\nRead metadata\n\nFor COGs, the query is registered with the pgSTAC database for the collection id and query parameters, such as variable and datetime.\nFor Zarr, the metadata is “lazily loaded” for the variable and temporal extent from the known collection store.\n\nGenerate tiles\n\nFor COGs, the mosaic ID returned from the registered query is used to read chunks from COGs on S3.\nFor Zarr, the metadata is used by xarray to read chunks from NetCDFs or Zarrs on S3 and the XarrayReader of rio_tiler is used to generate tiles.\n\n\nTo make as close to an “apples to apples” comparison as possible, we have stored COG metadata using pgSTAC for data in the nex-gddp-cmip6-cog bucket in AWS Relational Database Service (RDS) and Zarr metadata and data files in S3.\nTo profile the code for rendering tiles with either rio_tiler.XarrayReader or titiler-pgstac.PGSTACBackend, code was copied from those projects as needed to inject timers.\nThe time to generate a tile at various zoom levels was tested for each data store multiple times and box plots are used to report results for consistency with the dynamic client reporting.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarking Methodology"
    ]
  },
  {
    "objectID": "approaches/tiling/benchmarking-methodology.html#end-to-end-benchmarks",
    "href": "approaches/tiling/benchmarking-methodology.html#end-to-end-benchmarks",
    "title": "Benchmarking Methodology",
    "section": "End-to-End Benchmarks",
    "text": "End-to-End Benchmarks\nEnd-to-end tests provide benchmarks of response times for various tiles and datasets to titiler-xarray.\nDetails and code to generate the benchmarks and store the results on S3 is documented in tile-server-e2e-benchmarks.ipynb.\n\nEnd-to-End Benchmarks: Datasets\nA variety of datasets were selected for end-to-end testing and hopefully the framework makes it easy to modify and test new datasets and use cases come up. See the tile-server-e2e-benchmarks.ipynb for specific datasets.\n\n\nEnd-to-End Benchmarks: Approach\nCode from https://github.com/bdon/TileSiege was used to generate a set of tile URLs for the selected test datasets. The testing tool https://locust.io/ was used to run tests. Results are stored uploaded as CSV files in S3. These results are read and plotted in tile-server-e2e-benchmarks.ipynb.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarking Methodology"
    ]
  },
  {
    "objectID": "approaches/tiling/recommendations.html",
    "href": "approaches/tiling/recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "Data Format: At this time, COG + pgSTAC tiling performs better than tiling Zarr or kerchunk references, at all zoom levels.\nKerchunk Reference Files: The performance of tiling using a kerchunk reference can be as good or better than a zarr store. It is important to consider this is when the NetCDF files’ chunks are the same as the zarr store version.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Recommendations"
    ]
  },
  {
    "objectID": "approaches/tiling/recommendations.html#data-format",
    "href": "approaches/tiling/recommendations.html#data-format",
    "title": "Recommendations",
    "section": "",
    "text": "Data Format: At this time, COG + pgSTAC tiling performs better than tiling Zarr or kerchunk references, at all zoom levels.\nKerchunk Reference Files: The performance of tiling using a kerchunk reference can be as good or better than a zarr store. It is important to consider this is when the NetCDF files’ chunks are the same as the zarr store version.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Recommendations"
    ]
  },
  {
    "objectID": "approaches/tiling/recommendations.html#zarr-specific-recommendations",
    "href": "approaches/tiling/recommendations.html#zarr-specific-recommendations",
    "title": "Recommendations",
    "section": "Zarr-specific Recommendations",
    "text": "Zarr-specific Recommendations\n\nEnsure no zarr coordinate chunking: Ensure coordinate data is not being chunked. If coordinates are being chunked, it will result in more files being opened during xarray.open_dataset and cause significant performance degradation.\nSmaller chunk sizes perform better: Chunk size significantly impacts performance. A specific recommendation depends on the performance requirements of the application.\nFewer spatial chunks perform better: A greater number of chunks, spatially, will impact performance especially at low zoom levels as more chunks are loaded for greater spatial coverage.\nPyramids improve performance for high resolution datasets: High resolution datasets will suffer having either large chunks or many chunks, or both. To provide a good experience, zarr data can be aggregated into multiscale datasets, otherwise known as pyramids.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Recommendations"
    ]
  },
  {
    "objectID": "approaches/tiling/recommendations.html#what-is-high-resolution",
    "href": "approaches/tiling/recommendations.html#what-is-high-resolution",
    "title": "Recommendations",
    "section": "What is high resolution?",
    "text": "What is high resolution?\nGiven the current performance of titiler-xarray in tile-server-e2e-benchmarks.ipynb and assuming you are targeting 300ms or less, it would be suggested to target 8mb or smaller for your chunks.\nTo give a sense of what this means in terms of spatial resolution, and assuming a global dataset where the full spatial extent is stored in a single chunk, you would have the following dimensions of your dataset:\n\nimport numpy as np\n\ndatatypes = [\"float16\", \"float32\", \"float64\"]\ntotal_global_chunk_size_mb = 8\n\nfor data_type in datatypes:\n    # Determine the size in bytes of each data value\n    dtype = np.dtype(data_type)\n    # calculate the itemsize in megabytes\n    itemsize_mb = dtype.itemsize / 1024 / 1024\n    y_dim = np.sqrt(total_global_chunk_size_mb / 2 / itemsize_mb)\n    x_dim = y_dim * 2\n    x_deg = np.round(180 / y_dim, 3)\n    y_deg = np.round(360 / x_dim, 3)\n    # Source for lat/lon degrees conversion to meters: https://www.sco.wisc.edu/2022/01/21/how-big-is-a-degree/\n    deg_to_km = 111000 / 1000\n\n    print(f\"For data type {dtype}, an 8MB spatial dataset would have:\")\n    print(f\"* Dimensions: {int(np.round(y_dim))} x {int(np.round(x_dim))}\")\n    print(\n        f\"* Degrees for global data: {np.round(180 / y_dim, 3)} x {np.round(360 / x_dim, 3)}\"\n    )\n\n    # Some sources calculate that a degree of longitude at the equator is 111,319.5 meters, but this is just a ballpark figure for the spatial resolution.\n    print(\n        f\"* Approximate kilometers(km) at the equator: {int(np.round(deg_to_km * y_deg))} x {int(np.round(deg_to_km * x_deg, 0))}\\n\"\n    )\n\nFor data type float16, an 8MB spatial dataset would have:\n* Dimensions: 1448 x 2896\n* Degrees for global data: 0.124 x 0.124\n* Approximate kilometers(km) at the equator: 14 x 14\n\nFor data type float32, an 8MB spatial dataset would have:\n* Dimensions: 1024 x 2048\n* Degrees for global data: 0.176 x 0.176\n* Approximate kilometers(km) at the equator: 20 x 20\n\nFor data type float64, an 8MB spatial dataset would have:\n* Dimensions: 724 x 1448\n* Degrees for global data: 0.249 x 0.249\n* Approximate kilometers(km) at the equator: 28 x 28\n\n\n\nIf your dataset has a higher resolution than what is listed above, you will either want to chunk your data spatially or create a pyramid or both. Having spatially chunked data can also impact performance at low zoom levels, so you should try chunks significantly smaller than 8MB, say 4MB. Assuming the spatial extent of your data is larger than 16MB, you will probably want to create a pyramid.\nIt’s common to prefer larger chunk sizes for analysis workflows. These situations may motivate creating pyramids with small chunks for visualization purposes.\nThe Zarr V3 sharding extension may help in the future with the trade-off between chunk size and number of chunks at different zoom levels. Sharding stores multiple chunks together in one object, so range requests for small chunks will still work for high (zoomed in) zoom levels while the potential to concatenate adjacent ranges into a single request means multiple chunks or an entire shard could be read in one request for low (zoomed out) zoom levels.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Recommendations"
    ]
  },
  {
    "objectID": "approaches/tradeoffs.html",
    "href": "approaches/tradeoffs.html",
    "title": "Tradeoffs",
    "section": "",
    "text": "This page provides a very high-level summary of the pros and cons of the approaches detailed in this report. The tile server provides an API which is interoperable with multiple interfaces, but requires maintaining a tile server. also the response delivered to the client is an image format, not the raw data itself. The dynamic client approach has access to the underlying data and thus maximum flexibility in rendering and analysis for the user.",
    "crumbs": [
      "Approaches",
      "Tradeoffs"
    ]
  },
  {
    "objectID": "approaches/tradeoffs.html#dynamic-client---pros",
    "href": "approaches/tradeoffs.html#dynamic-client---pros",
    "title": "Tradeoffs",
    "section": "Dynamic Client - Pros",
    "text": "Dynamic Client - Pros\n\nYou don’t have to run a server.\nNumeric data is available for time series, color mapping, spatial aggregation, etc.",
    "crumbs": [
      "Approaches",
      "Tradeoffs"
    ]
  },
  {
    "objectID": "approaches/tradeoffs.html#dynamic-client---cons",
    "href": "approaches/tradeoffs.html#dynamic-client---cons",
    "title": "Tradeoffs",
    "section": "Dynamic Client - Cons",
    "text": "Dynamic Client - Cons\n\nA custom interface makes it challenging to integrate other data sources.\nHas pre-processing requirements such as pyramid generation.",
    "crumbs": [
      "Approaches",
      "Tradeoffs"
    ]
  },
  {
    "objectID": "approaches/tradeoffs.html#tile-server---pros",
    "href": "approaches/tradeoffs.html#tile-server---pros",
    "title": "Tradeoffs",
    "section": "Tile server - Pros",
    "text": "Tile server - Pros\n\nThe tile server serves standard tiling APIs (xyz and tiles) tiling APIs. These APIs integrate with various map platforms and can take advantage of their features (ex. Mapbox v2 projection).\nCan be used with anything that is xarray-readable, including kerchunk reference files, NetCDF and HDF5.",
    "crumbs": [
      "Approaches",
      "Tradeoffs"
    ]
  },
  {
    "objectID": "approaches/tradeoffs.html#tile-server---cons",
    "href": "approaches/tradeoffs.html#tile-server---cons",
    "title": "Tradeoffs",
    "section": "Tile server - Cons",
    "text": "Tile server - Cons\n\nServes image tiles, not data.",
    "crumbs": [
      "Approaches",
      "Tradeoffs"
    ]
  },
  {
    "objectID": "approaches/tiling.html",
    "href": "approaches/tiling.html",
    "title": "Tiling",
    "section": "",
    "text": "An xarray tile server provides image tiles via the XYZ Protocol and OGC Tiles API specifications.\nThe tile server approach relies on the rio_tiler.XarrayReader library which includes the tile function. This module supports tiling of anything that is xarray-readable, so a tile server using this library can render tiles from Zarr stores as well as netCDF4/HDF5 and other formats. An example API infrastructure can be found in titiler-multidim. Please note this library is still in development and is not intended for production use at this time.\nUsers can preview their zarr data using the map form and preview tool at https://staging.openveda.cloud/api/multidim/WebMercatorQuad/map.",
    "crumbs": [
      "Approaches",
      "Tiling"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zarr Visualization Report",
    "section": "",
    "text": "This site documents different approaches and benchmarks for Zarr visualization. If you use or create Zarr data and wish to visualize it in a web browser, this guide is for you. It describes the requirements for data pre-processing and chunking in order to support visualization through tiling server and dynamic client approaches.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#pre-generated-map-tiles---drawbacks",
    "href": "index.html#pre-generated-map-tiles---drawbacks",
    "title": "Zarr Visualization Report",
    "section": "Pre-generated Map Tiles - Drawbacks",
    "text": "Pre-generated Map Tiles - Drawbacks\nThe challenge of visualizing large geospatial datasets led to the development of pre-generated static map tiles. While pregenerated map tiles make it possible to visualize data quickly, there are drawbacks. The most significant drawback is the data provider chooses how the data will appear. Next generation approaches give that power to the user. Other drawbacks impact the data provider, such as storage costs and maintaining a pipeline to constantly update or reprocess the tile storage with new and updated data. But the user is impacted by having no power to adjust the visualization, such as modifying the color scale, color map or perform “band math” where multiple variables are combined to produce a new variable.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#new-approaches",
    "href": "index.html#new-approaches",
    "title": "Zarr Visualization Report",
    "section": "New Approaches",
    "text": "New Approaches\nMore recent years have seen the success of the dynamic tiling approach which allows for on-demand map tile creation. This approach has traditionally relied on reading data from Cloud-Optimized GeoTIFFs (COGs). When the Zarr data format gained popularity for large-scale N-dimensional data analysis, users started to ask for browser-based visualization. The conventional Zarr chunk size stored for analysis (~100MB) was acknowledged to be too large to be fetched by a browser.\nNow there are two options for visualizing Zarr data: a tile server and dynamic client. rio_tiler’s XarrayReader supports tile rendering from anything that is xarray-readable. This means a tile server can render tiles from Zarr stores as well as netCDF4/HDF5 and other formats. However, a tile server still requires running a server while the second option, “dynamic client”, reads Zarr directly in the browser client and uses webGL to render map tiles.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Zarr Visualization Report",
    "section": "Goals",
    "text": "Goals\nThis report will describe these two approaches. We will discuss the tradeoffs, pre-processing options and provide performance benchmarks for a variety of data configurations. We hope readers will be able to reuse lessons learned and recommendations to deliver their Zarr data to users in web browser and contribute to the wider adoption of this format for large scale environmental data understanding.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "transition-from-rmarkdown.html",
    "href": "transition-from-rmarkdown.html",
    "title": "Transition from RMarkdown",
    "section": "",
    "text": "You may already have workflows in RMarkdown and are interested in transitioning to Quarto. There’s no hurry to migrate to Quarto. Keep using Rmarkdown and when you’re ready the migration will be fine.\nHere are some notes as we migrate RMarkdown sites and books.\nTODO: translating R code chunks"
  },
  {
    "objectID": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "href": "transition-from-rmarkdown.html#bookdown-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Bookdown to Quarto",
    "text": "Bookdown to Quarto\nConverting a Bookdown book to Quarto is slightly more involved than converting a website. A book has chapters whose order must be defined, and likely has citations and cross-refs. Still, conversion is not that hard.\nWe got some practice converting from Bookdown to Quarto by helping Gavin Fay convert his lab’s fantastic onboarding documentation, the Faylab Lab Manual. Here’s the GitHub view before and after.\nOur best first reference material for this was Nick Tierney’s Notes on Changing from Rmarkdown/Bookdown to Quarto. Nick shares some scripts in that post to automate some changes. In our case, the book was small enough that we made all changes manually. Quarto documentation was indispensable.\n\nExperimenting in a low-risk environment\nWe forked a copy of the Faylab Lab manual to the Openscapes organization, and worked in a branch so we could make changes relatively risk-free. We could always fork a new copy of the original if we “broke” something. (Caution: the default when making a pull request from a fork is to push changes to the original upstream repo, not your fork and it does this without warning if you have write-access to that repo.) With local previews it’s easy to test / play with settings to see what they do. We tended to make a change, Preview, then compare the look and functionality of the book to the original. It was helpful to comment out some elements of the configuration file _output.yml after their counterparts had been added to the Quarto configuration file _quarto.yml, or to confirm they were no longer needed, before making the drastic move of deleting them.\n\n\nThe conversion\nHere are the main steps to convert the Faylab Lab manual from Bookdown to Quarto.\nCreate new empty file called _quarto.yml and add book metadata there. The screenshots below\nSet project type as book.\nMove metadata out of index.qmd and into _quarto.yml. Title, author, and publication date were in index.qmd with date set using date: \"Last updated:r Sys.Date()\". Now these are in _quarto.yml with date set using date: last-modified. Note that having R code would require you to adjust code chunk options in the Quarto style (#|). This tripped us up a bit; see GitHub Actions.\nMove chapters listing out of _bookdown.yml and into _quarto.yml.\nAdd page footer to _quarto.yml.\nHere’s what ours looked like when we finished the steps above (_quarto.yml).\n\n\n\n\n\n\n\n\n\n_quarto.yml contents\n\n\n\n\n\n\n\nFaylab Lab Manual\n\n\n\n\n\nChange insertion of images from html style to Quarto style. (Note Quarto calls them “figures”, not “images”.) The following snippet will insert the GitHub octocat logo in a page:\n![](https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png){fig-align=\"left\" width=\"35px\"}\nChange all filename extensions .Rmd -&gt; .qmd (you could Preview after this change and see that the book looks the same). Note that Quarto works with .Rmd files just as well as it does .qmd, so this change is not urgent. In fact, if you have a lot of R code in your .Rmds (unlike the Faylab Lab Manual), there will be additional tinkering needed to make the code chunks happy.\n\n\nCitations\nThe Faylab Lab Manual cited two papers, presenting us with an opportunity to see how easy it is to add references to a Quarto book. Briefly, in the Visual Editor, Insert &gt; Citation &gt; DOI. Pasting the DOI or its full URL, we can insert the citation. This automatically creates a references.bib file and adds the full citations at the bottom of the chapter page (watch demo). In July 2022, we had to manually add a ## References heading, but this may not be necessary in future Quarto updates.\n\n\n\n\n\n\n\n\n\nInsert citation via its DOI using RStudio Visual Editor\n\n\n\n\n\n\n\n\n\n\nPublishing notes\nIf the book’s output is strictly html, there’s no need to specify output-dir in _quarto.yml. The output directory default is _book/, which is what we’d like. If we wanted other types of output like like PDF or EPUB, etc. those single file outputs are also written to the output-dir (Quarto docs).\nIf you currently have a docs/ folder, delete it.\nUpdate .gitignore to ignore _book/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_book/\nOnce all is settled, delete _output.yml.\nOnce the Openscapes fork was fully reviewed, we made a pull request from that to the main branch of the book’s repo. Once that was merged, we set up GitHub Actions to render the book. (TODO: instructions for GitHub Actions)\n\n\nGitHub Actions\nThis book was mostly prose and screenshots without any R code. This made the conversion from RMarkdown to Quarto likely more straightforward than if you also needed to adjust code chunk options in the quarto style (#|). Our initial GitHub Action to render the converted Faylab Lab Manual failed because we had a piece of R code - even though the code was commented out! This was resolved when we deleted the line."
  },
  {
    "objectID": "transition-from-rmarkdown.html#distill-to-quarto",
    "href": "transition-from-rmarkdown.html#distill-to-quarto",
    "title": "Transition from RMarkdown",
    "section": "Distill to quarto",
    "text": "Distill to quarto\nWe transitioned our events site from distill to quarto in May 2022 (github view before and after). We followed excellent notes and examples from Nick Tierney and Danielle Navarro.\nAfter we had changed all the files, the Build tab in the RStudio IDE still showed “Build website” rather then “Render Website” and “Preview Website”, and would error when we pushed them (because that button was expecting a distill site, not a quarto site). To fix this, we updated the .Rproj file. Clicking on the .Rproj file in the RStudio IDE will open a dialog box where you can click things you want (you can also open these in a text editor or from the GitHub website to see the actual text). To fix this situation with the Build tab: Project Options &gt; Build Tools &gt; Project Build Tools &gt; None.\nLooking at files /posts/_metadata.yml and _quarto.yml helps see where things are defined. For example, to make event post citations appear, we added citation: true to /posts/_metadata.yml and in _quarto.yml under the website key we set site-url: https://openscapes.github.io/events. We deleted footer.html used with distill because footer is now defined in quarto.yml.\n\nPublishing notes\n\nBackground: Our distill site had been set up to output to a docs folder, and had GitHub Settings &gt; Pages set to look there rather gh-pages branch. (Julie note: this was a new-to-me capability when we set up the events distill site in Spring 2021 so I had forgotten that was an option). We’ve initially kept this same set-up for now with our events page in _quarto.yml: output-dir: docs. However, this is sub-optimal; better to not have to commit and push these files but to instead have a GitHub Action generate them upon a commit. So the following is what we did -\n\nDon’t specify output-dir in _quarto.yml. The output directory default is _site/, which is what we’d like.\nIf you currently have a docs/ folder (like we did as we were experimenting), delete it.\nUpdate .gitignore to ignore _site/. At the same time, we have it ignore caches and a .quarto file:\n/.quarto/\n*_cache/\n_site/\nPush these changes, merge into main.\nOn GitHub.com, in your repo, set up GitHub publishing\nFollow instructions from the explore and setup chapter."
  },
  {
    "objectID": "transition-from-rmarkdown.html#troubleshooting",
    "href": "transition-from-rmarkdown.html#troubleshooting",
    "title": "Transition from RMarkdown",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nGitHub Action fails, says you need RMarkdown but you don’t have R code!\nAnd you changed all .Rmds to .qmds!\nYou likely have a few setup code chunks from RMarkdown, that look like this:\n{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\nYou can find them by opening each of your files and having a look, or use GitHub’s search for the keyword knitr"
  },
  {
    "objectID": "approaches/index.html",
    "href": "approaches/index.html",
    "title": "Approaches",
    "section": "",
    "text": "For browser-based visualization of Zarr, there are 2 approaches covered in this report:\n\nTiling\nDynamic client",
    "crumbs": [
      "Approaches"
    ]
  },
  {
    "objectID": "approaches/tiling/tile-server-e2e-benchmarks.html",
    "href": "approaches/tiling/tile-server-e2e-benchmarks.html",
    "title": "Tile Server End to End Benchmarks",
    "section": "",
    "text": "In addition to application code benchmarks, it is important have e2e benchmarks which demonstrate the performance of the tile server. Running end-to-end benchmarks is documented in https://github.com/developmentseed/tile-benchmarking/tree/main/03-e2e/README.md.\nTested tiles ranged from zoom 0 to zoom 5. From testing we noticed that the most important factors in determining response time were the number of coordinate chunks and the chunk size. The time reported is the median response time and represents the median response time for multiple requests (usually 10) for the same tile (i.e. x, y, and z parameters).\nBased on this, the recommendation would be to create chunks as small as possible (e.g. 1 timestep per chunk for the full spatial extent if visualization is the main goal) and not to chunk coordinate data.\nBelow, we include an example of how to plot results from one execution of these benchmarks.\nFirst we download the results:\nimport sys\n\nsys.path.append(\"../../../tile-benchmarking/helpers/\")\nimport eodc_hub_role\n\ncredentials = eodc_hub_role.fetch_and_set_credentials()\n%%capture\n# download results from s3 or copy them from the tile-benchmarking repo\n!aws s3 cp --recursive s3://nasa-eodc-data-store/tile-benchmarking-results/2023-10-08_00-15-09/ downloaded_results/\n# Import libraries\nimport os\nimport pandas as pd\nimport holoviews as hv\nimport warnings\n\npd.options.plotting.backend = \"holoviews\"\n\nwarnings.filterwarnings(\"ignore\")\nParse and merge results into a single dataframe.\n# Specify the directory path and the suffix\ndirectory_path = \"downloaded_results/\"\nsuffix = \"_urls_stats.csv\"  # For example, if you're interested in text files\n\n# List all files in the directory\nall_files = os.listdir(directory_path)\n\n# Filter the files to only include those that end with the specified suffix\nfiles_with_suffix = [f\"{directory_path}{f}\" for f in all_files if f.endswith(suffix)]\n\ndfs = []\nfor file in files_with_suffix:\n    df = pd.read_csv(file)\n    df[\"file\"] = file\n    dfs.append(df)\n\nmerged_df = pd.concat(dfs)\nThe “Name” column is either the URL path (i.e. /0/0/0) OR “Aggregated”. The “Aggregated” results represent aggregations across tile endpoints. We want to understand results by zoom level, so the “Aggregated” rows are removed and we parse the zoom from the tile endpoint. We also parse the dataset name from the file name.\ndf_filtered = merged_df[merged_df[\"Name\"] != \"Aggregated\"]\ndf_filtered[\"zoom\"] = [int(path.split(\"/\")[2]) for path in df_filtered[\"Name\"]]\ndf_filtered[\"dataset\"] = [\n    file.split(\"/\")[1].replace(\"_urls_stats.csv\", \"\") for file in df_filtered[\"file\"]\n]\nNext, we load and merge the zoom_info.csv file that was generated when generating the URLs to test (see gen_test_urls.py, so that we can view performance alongside characteristics of the dataset, like chunk size.\ndf_filtered_min = df_filtered[[\"dataset\", \"zoom\", \"Median Response Time\"]]\nzarr_info = pd.read_csv(\"../../../tile-benchmarking/03-e2e/zarr_info.csv\")\nzarr_info_min = zarr_info[\n    [\n        \"collection_name\",\n        \"shape_dict\",\n        \"chunks\",\n        \"chunk_size_mb\",\n        \"number_of_spatial_chunks\",\n        \"dtype\",\n    ]\n]\n# power_901_monthly_meteorology_utc is a bit of an outlier\nzarr_info_min = zarr_info_min[\n    zarr_info_min[\"collection_name\"] != \"power_901_monthly_meteorology_utc.zarr\"\n]\nresults_info_merged = zarr_info_min.merge(\n    df_filtered_min, left_on=\"collection_name\", right_on=\"dataset\", how=\"outer\"\n)",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Tile Server End to End Benchmarks"
    ]
  },
  {
    "objectID": "approaches/tiling/tile-server-e2e-benchmarks.html#table-of-responses",
    "href": "approaches/tiling/tile-server-e2e-benchmarks.html#table-of-responses",
    "title": "Tile Server End to End Benchmarks",
    "section": "Table of responses",
    "text": "Table of responses\nIt may be easier to understand these results in a table. Because multiple tiles were tested above zoom 0 (which only has one tile) we can take the average response across tiles at a given zoom level and put those results in a table alongside the dependent variables of chunk size and number of spatial chunks for easier interpretation.\n\nfor zoom in range(6):\n    display(f\"Results for Zoom {zoom}\")\n    this_zoom_results = results_info_merged[results_info_merged[\"zoom\"] == zoom].drop(\n        columns=[\"dataset\"]\n    )\n    average_response = (\n        this_zoom_results.groupby(\"collection_name\")[\"Median Response Time\"]\n        .mean()\n        .reset_index()\n    )\n    average_response.rename(\n        columns={\"Median Response Time\": \"Average Median Response\"}, inplace=True\n    )\n\n    # Merge this with the original dataframe\n    result = pd.merge(average_response, zarr_info_min, on=\"collection_name\", how=\"left\")\n\n    display(result.sort_values([\"Average Median Response\"]))\n\n'Results for Zoom 0'\n\n\n\n\n\n\n\n\n\ncollection_name\nAverage Median Response\nshape_dict\nchunks\nchunk_size_mb\nnumber_of_spatial_chunks\ndtype\n\n\n\n\n10\nsingle_chunk_store_lat512_lon1024.zarr\n220.000000\n{'time': 1, 'lat': 512, 'lon': 1024}\n{'time': 1, 'lat': 512, 'lon': 1024}\n4.000000\n1.000000\nfloat64\n\n\n1\n600_1440_1_CMIP6_daily_GISS-E2-1-G_tas.zarr\n270.000000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n11\nsingle_chunk_store_lat724_lon1448.zarr\n280.000000\n{'time': 1, 'lat': 724, 'lon': 1448}\n{'time': 1, 'lat': 724, 'lon': 1448}\n7.998291\n1.000000\nfloat64\n\n\n3\ncmip6-kerchunk\n330.000000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n5\nsingle_chunk_store_lat1024_lon2048.zarr\n350.000000\n{'time': 1, 'lat': 1024, 'lon': 2048}\n{'time': 1, 'lat': 1024, 'lon': 2048}\n16.000000\n1.000000\nfloat64\n\n\n4\ncmip6-pds_GISS-E2-1-G_historical_tas\n510.000000\n{'time': 1980, 'lat': 90, 'lon': 144}\n{'time': 600, 'lat': 90, 'lon': 144}\n29.663086\n1.000000\nfloat32\n\n\n6\nsingle_chunk_store_lat1448_lon2896.zarr\n510.000000\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n2\n600_1440_29_CMIP6_daily_GISS-E2-1-G_tas.zarr\n540.000000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 29, 'lat': 600, 'lon': 1440}\n95.581055\n1.000000\nfloat32\n\n\n12\nwith_chunks_store_lat1448_lon2896.zarr\n550.000000\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n13\nwith_chunks_store_lat2048_lon4096.zarr\n630.000000\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n2.000427\nfloat64\n\n\n14\nwith_chunks_store_lat2896_lon5792.zarr\n840.000000\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n4.000000\nfloat64\n\n\n7\nsingle_chunk_store_lat2048_lon4096.zarr\n980.000000\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 2048, 'lon': 4096}\n64.000000\n1.000000\nfloat64\n\n\n8\nsingle_chunk_store_lat2896_lon5792.zarr\n1704.102722\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 2896, 'lon': 5792}\n127.972656\n1.000000\nfloat64\n\n\n15\nwith_chunks_store_lat4096_lon8192.zarr\n1900.000000\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n8.001709\nfloat64\n\n\n0\n365_262_262_CMIP6_daily_GISS-E2-1-G_tas.zarr\n3200.000000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 365, 'lat': 262, 'lon': 262}\n95.577469\n12.586679\nfloat32\n\n\n9\nsingle_chunk_store_lat4096_lon8192.zarr\n3700.000000\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 4096, 'lon': 8192}\n256.000000\n1.000000\nfloat64\n\n\n16\nwith_chunks_store_lat5793_lon11586.zarr\n5700.000000\n{'time': 1, 'lat': 5793, 'lon': 11586}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n16.005525\nfloat64\n\n\n\n\n\n\n\n'Results for Zoom 1'\n\n\n\n\n\n\n\n\n\ncollection_name\nAverage Median Response\nshape_dict\nchunks\nchunk_size_mb\nnumber_of_spatial_chunks\ndtype\n\n\n\n\n10\nsingle_chunk_store_lat512_lon1024.zarr\n220.0\n{'time': 1, 'lat': 512, 'lon': 1024}\n{'time': 1, 'lat': 512, 'lon': 1024}\n4.000000\n1.000000\nfloat64\n\n\n3\ncmip6-kerchunk\n225.0\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n11\nsingle_chunk_store_lat724_lon1448.zarr\n250.0\n{'time': 1, 'lat': 724, 'lon': 1448}\n{'time': 1, 'lat': 724, 'lon': 1448}\n7.998291\n1.000000\nfloat64\n\n\n1\n600_1440_1_CMIP6_daily_GISS-E2-1-G_tas.zarr\n260.0\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n5\nsingle_chunk_store_lat1024_lon2048.zarr\n330.0\n{'time': 1, 'lat': 1024, 'lon': 2048}\n{'time': 1, 'lat': 1024, 'lon': 2048}\n16.000000\n1.000000\nfloat64\n\n\n6\nsingle_chunk_store_lat1448_lon2896.zarr\n485.0\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n12\nwith_chunks_store_lat1448_lon2896.zarr\n490.0\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n4\ncmip6-pds_GISS-E2-1-G_historical_tas\n500.0\n{'time': 1980, 'lat': 90, 'lon': 144}\n{'time': 600, 'lat': 90, 'lon': 144}\n29.663086\n1.000000\nfloat32\n\n\n13\nwith_chunks_store_lat2048_lon4096.zarr\n520.0\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n2.000427\nfloat64\n\n\n2\n600_1440_29_CMIP6_daily_GISS-E2-1-G_tas.zarr\n540.0\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 29, 'lat': 600, 'lon': 1440}\n95.581055\n1.000000\nfloat32\n\n\n14\nwith_chunks_store_lat2896_lon5792.zarr\n610.0\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n4.000000\nfloat64\n\n\n7\nsingle_chunk_store_lat2048_lon4096.zarr\n845.0\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 2048, 'lon': 4096}\n64.000000\n1.000000\nfloat64\n\n\n0\n365_262_262_CMIP6_daily_GISS-E2-1-G_tas.zarr\n1030.0\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 365, 'lat': 262, 'lon': 262}\n95.577469\n12.586679\nfloat32\n\n\n15\nwith_chunks_store_lat4096_lon8192.zarr\n1115.0\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n8.001709\nfloat64\n\n\n16\nwith_chunks_store_lat5793_lon11586.zarr\n1300.0\n{'time': 1, 'lat': 5793, 'lon': 11586}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n16.005525\nfloat64\n\n\n8\nsingle_chunk_store_lat2896_lon5792.zarr\n1500.0\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 2896, 'lon': 5792}\n127.972656\n1.000000\nfloat64\n\n\n9\nsingle_chunk_store_lat4096_lon8192.zarr\n2900.0\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 4096, 'lon': 8192}\n256.000000\n1.000000\nfloat64\n\n\n\n\n\n\n\n'Results for Zoom 2'\n\n\n\n\n\n\n\n\n\ncollection_name\nAverage Median Response\nshape_dict\nchunks\nchunk_size_mb\nnumber_of_spatial_chunks\ndtype\n\n\n\n\n3\ncmip6-kerchunk\n218.000000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n10\nsingle_chunk_store_lat512_lon1024.zarr\n220.000000\n{'time': 1, 'lat': 512, 'lon': 1024}\n{'time': 1, 'lat': 512, 'lon': 1024}\n4.000000\n1.000000\nfloat64\n\n\n1\n600_1440_1_CMIP6_daily_GISS-E2-1-G_tas.zarr\n254.000000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n11\nsingle_chunk_store_lat724_lon1448.zarr\n258.333333\n{'time': 1, 'lat': 724, 'lon': 1448}\n{'time': 1, 'lat': 724, 'lon': 1448}\n7.998291\n1.000000\nfloat64\n\n\n5\nsingle_chunk_store_lat1024_lon2048.zarr\n325.000000\n{'time': 1, 'lat': 1024, 'lon': 2048}\n{'time': 1, 'lat': 1024, 'lon': 2048}\n16.000000\n1.000000\nfloat64\n\n\n13\nwith_chunks_store_lat2048_lon4096.zarr\n408.333333\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n2.000427\nfloat64\n\n\n6\nsingle_chunk_store_lat1448_lon2896.zarr\n482.000000\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n4\ncmip6-pds_GISS-E2-1-G_historical_tas\n486.666667\n{'time': 1980, 'lat': 90, 'lon': 144}\n{'time': 600, 'lat': 90, 'lon': 144}\n29.663086\n1.000000\nfloat32\n\n\n12\nwith_chunks_store_lat1448_lon2896.zarr\n486.666667\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n2\n600_1440_29_CMIP6_daily_GISS-E2-1-G_tas.zarr\n510.000000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 29, 'lat': 600, 'lon': 1440}\n95.581055\n1.000000\nfloat32\n\n\n14\nwith_chunks_store_lat2896_lon5792.zarr\n610.000000\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n4.000000\nfloat64\n\n\n15\nwith_chunks_store_lat4096_lon8192.zarr\n817.500000\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n8.001709\nfloat64\n\n\n7\nsingle_chunk_store_lat2048_lon4096.zarr\n842.500000\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 2048, 'lon': 4096}\n64.000000\n1.000000\nfloat64\n\n\n0\n365_262_262_CMIP6_daily_GISS-E2-1-G_tas.zarr\n852.500000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 365, 'lat': 262, 'lon': 262}\n95.577469\n12.586679\nfloat32\n\n\n16\nwith_chunks_store_lat5793_lon11586.zarr\n1118.333333\n{'time': 1, 'lat': 5793, 'lon': 11586}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n16.005525\nfloat64\n\n\n8\nsingle_chunk_store_lat2896_lon5792.zarr\n1500.000000\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 2896, 'lon': 5792}\n127.972656\n1.000000\nfloat64\n\n\n9\nsingle_chunk_store_lat4096_lon8192.zarr\n2975.000000\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 4096, 'lon': 8192}\n256.000000\n1.000000\nfloat64\n\n\n\n\n\n\n\n'Results for Zoom 3'\n\n\n\n\n\n\n\n\n\ncollection_name\nAverage Median Response\nshape_dict\nchunks\nchunk_size_mb\nnumber_of_spatial_chunks\ndtype\n\n\n\n\n3\ncmip6-kerchunk\n208.181818\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n10\nsingle_chunk_store_lat512_lon1024.zarr\n214.545455\n{'time': 1, 'lat': 512, 'lon': 1024}\n{'time': 1, 'lat': 512, 'lon': 1024}\n4.000000\n1.000000\nfloat64\n\n\n1\n600_1440_1_CMIP6_daily_GISS-E2-1-G_tas.zarr\n245.454545\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n11\nsingle_chunk_store_lat724_lon1448.zarr\n255.833333\n{'time': 1, 'lat': 724, 'lon': 1448}\n{'time': 1, 'lat': 724, 'lon': 1448}\n7.998291\n1.000000\nfloat64\n\n\n5\nsingle_chunk_store_lat1024_lon2048.zarr\n326.666667\n{'time': 1, 'lat': 1024, 'lon': 2048}\n{'time': 1, 'lat': 1024, 'lon': 2048}\n16.000000\n1.000000\nfloat64\n\n\n13\nwith_chunks_store_lat2048_lon4096.zarr\n406.363636\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n2.000427\nfloat64\n\n\n6\nsingle_chunk_store_lat1448_lon2896.zarr\n484.545455\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n12\nwith_chunks_store_lat1448_lon2896.zarr\n484.545455\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n14\nwith_chunks_store_lat2896_lon5792.zarr\n485.555556\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n4.000000\nfloat64\n\n\n4\ncmip6-pds_GISS-E2-1-G_historical_tas\n497.272727\n{'time': 1980, 'lat': 90, 'lon': 144}\n{'time': 600, 'lat': 90, 'lon': 144}\n29.663086\n1.000000\nfloat32\n\n\n2\n600_1440_29_CMIP6_daily_GISS-E2-1-G_tas.zarr\n512.000000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 29, 'lat': 600, 'lon': 1440}\n95.581055\n1.000000\nfloat32\n\n\n15\nwith_chunks_store_lat4096_lon8192.zarr\n554.166667\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n8.001709\nfloat64\n\n\n0\n365_262_262_CMIP6_daily_GISS-E2-1-G_tas.zarr\n647.272727\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 365, 'lat': 262, 'lon': 262}\n95.577469\n12.586679\nfloat32\n\n\n16\nwith_chunks_store_lat5793_lon11586.zarr\n655.000000\n{'time': 1, 'lat': 5793, 'lon': 11586}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n16.005525\nfloat64\n\n\n7\nsingle_chunk_store_lat2048_lon4096.zarr\n838.333333\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 2048, 'lon': 4096}\n64.000000\n1.000000\nfloat64\n\n\n8\nsingle_chunk_store_lat2896_lon5792.zarr\n1500.000000\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 2896, 'lon': 5792}\n127.972656\n1.000000\nfloat64\n\n\n9\nsingle_chunk_store_lat4096_lon8192.zarr\n3037.671648\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 4096, 'lon': 8192}\n256.000000\n1.000000\nfloat64\n\n\n\n\n\n\n\n'Results for Zoom 4'\n\n\n\n\n\n\n\n\n\ncollection_name\nAverage Median Response\nshape_dict\nchunks\nchunk_size_mb\nnumber_of_spatial_chunks\ndtype\n\n\n\n\n3\ncmip6-kerchunk\n200.666667\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n10\nsingle_chunk_store_lat512_lon1024.zarr\n214.666667\n{'time': 1, 'lat': 512, 'lon': 1024}\n{'time': 1, 'lat': 512, 'lon': 1024}\n4.000000\n1.000000\nfloat64\n\n\n1\n600_1440_1_CMIP6_daily_GISS-E2-1-G_tas.zarr\n240.000000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n11\nsingle_chunk_store_lat724_lon1448.zarr\n253.571429\n{'time': 1, 'lat': 724, 'lon': 1448}\n{'time': 1, 'lat': 724, 'lon': 1448}\n7.998291\n1.000000\nfloat64\n\n\n5\nsingle_chunk_store_lat1024_lon2048.zarr\n328.461538\n{'time': 1, 'lat': 1024, 'lon': 2048}\n{'time': 1, 'lat': 1024, 'lon': 2048}\n16.000000\n1.000000\nfloat64\n\n\n13\nwith_chunks_store_lat2048_lon4096.zarr\n410.000000\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n2.000427\nfloat64\n\n\n15\nwith_chunks_store_lat4096_lon8192.zarr\n453.571429\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n8.001709\nfloat64\n\n\n6\nsingle_chunk_store_lat1448_lon2896.zarr\n479.333333\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n14\nwith_chunks_store_lat2896_lon5792.zarr\n480.666667\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n4.000000\nfloat64\n\n\n12\nwith_chunks_store_lat1448_lon2896.zarr\n486.428571\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n4\ncmip6-pds_GISS-E2-1-G_historical_tas\n504.285714\n{'time': 1980, 'lat': 90, 'lon': 144}\n{'time': 600, 'lat': 90, 'lon': 144}\n29.663086\n1.000000\nfloat32\n\n\n2\n600_1440_29_CMIP6_daily_GISS-E2-1-G_tas.zarr\n509.333333\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 29, 'lat': 600, 'lon': 1440}\n95.581055\n1.000000\nfloat32\n\n\n16\nwith_chunks_store_lat5793_lon11586.zarr\n532.666667\n{'time': 1, 'lat': 5793, 'lon': 11586}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n16.005525\nfloat64\n\n\n0\n365_262_262_CMIP6_daily_GISS-E2-1-G_tas.zarr\n577.500000\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 365, 'lat': 262, 'lon': 262}\n95.577469\n12.586679\nfloat32\n\n\n7\nsingle_chunk_store_lat2048_lon4096.zarr\n838.666667\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 2048, 'lon': 4096}\n64.000000\n1.000000\nfloat64\n\n\n8\nsingle_chunk_store_lat2896_lon5792.zarr\n1500.000000\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 2896, 'lon': 5792}\n127.972656\n1.000000\nfloat64\n\n\n9\nsingle_chunk_store_lat4096_lon8192.zarr\n3008.821526\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 4096, 'lon': 8192}\n256.000000\n1.000000\nfloat64\n\n\n\n\n\n\n\n'Results for Zoom 5'\n\n\n\n\n\n\n\n\n\ncollection_name\nAverage Median Response\nshape_dict\nchunks\nchunk_size_mb\nnumber_of_spatial_chunks\ndtype\n\n\n\n\n3\ncmip6-kerchunk\n208.148148\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n10\nsingle_chunk_store_lat512_lon1024.zarr\n214.444444\n{'time': 1, 'lat': 512, 'lon': 1024}\n{'time': 1, 'lat': 512, 'lon': 1024}\n4.000000\n1.000000\nfloat64\n\n\n1\n600_1440_1_CMIP6_daily_GISS-E2-1-G_tas.zarr\n240.740741\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 1, 'lat': 600, 'lon': 1440}\n3.295898\n1.000000\nfloat32\n\n\n11\nsingle_chunk_store_lat724_lon1448.zarr\n251.851852\n{'time': 1, 'lat': 724, 'lon': 1448}\n{'time': 1, 'lat': 724, 'lon': 1448}\n7.998291\n1.000000\nfloat64\n\n\n5\nsingle_chunk_store_lat1024_lon2048.zarr\n330.740741\n{'time': 1, 'lat': 1024, 'lon': 2048}\n{'time': 1, 'lat': 1024, 'lon': 2048}\n16.000000\n1.000000\nfloat64\n\n\n13\nwith_chunks_store_lat2048_lon4096.zarr\n393.703704\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n2.000427\nfloat64\n\n\n15\nwith_chunks_store_lat4096_lon8192.zarr\n461.111111\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n8.001709\nfloat64\n\n\n6\nsingle_chunk_store_lat1448_lon2896.zarr\n480.370370\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n14\nwith_chunks_store_lat2896_lon5792.zarr\n482.800000\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n4.000000\nfloat64\n\n\n12\nwith_chunks_store_lat1448_lon2896.zarr\n483.461538\n{'time': 1, 'lat': 1448, 'lon': 2896}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n1.000000\nfloat64\n\n\n16\nwith_chunks_store_lat5793_lon11586.zarr\n493.461538\n{'time': 1, 'lat': 5793, 'lon': 11586}\n{'time': 1, 'lat': 1448, 'lon': 2896}\n31.993164\n16.005525\nfloat64\n\n\n4\ncmip6-pds_GISS-E2-1-G_historical_tas\n497.777778\n{'time': 1980, 'lat': 90, 'lon': 144}\n{'time': 600, 'lat': 90, 'lon': 144}\n29.663086\n1.000000\nfloat32\n\n\n2\n600_1440_29_CMIP6_daily_GISS-E2-1-G_tas.zarr\n511.851852\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 29, 'lat': 600, 'lon': 1440}\n95.581055\n1.000000\nfloat32\n\n\n0\n365_262_262_CMIP6_daily_GISS-E2-1-G_tas.zarr\n614.814815\n{'time': 730, 'lat': 600, 'lon': 1440}\n{'time': 365, 'lat': 262, 'lon': 262}\n95.577469\n12.586679\nfloat32\n\n\n7\nsingle_chunk_store_lat2048_lon4096.zarr\n835.925926\n{'time': 1, 'lat': 2048, 'lon': 4096}\n{'time': 1, 'lat': 2048, 'lon': 4096}\n64.000000\n1.000000\nfloat64\n\n\n8\nsingle_chunk_store_lat2896_lon5792.zarr\n1500.000000\n{'time': 1, 'lat': 2896, 'lon': 5792}\n{'time': 1, 'lat': 2896, 'lon': 5792}\n127.972656\n1.000000\nfloat64\n\n\n9\nsingle_chunk_store_lat4096_lon8192.zarr\n3033.333333\n{'time': 1, 'lat': 4096, 'lon': 8192}\n{'time': 1, 'lat': 4096, 'lon': 8192}\n256.000000\n1.000000\nfloat64",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Tile Server End to End Benchmarks"
    ]
  },
  {
    "objectID": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html",
    "href": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html",
    "title": "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables",
    "section": "",
    "text": "This notebook does not report any results for tiling Zarr datasets. It surfaces the significance of the underlying environment and configuration of low level libraries to the performance of a framework we are comparing with for tiling imagery.\ntitiler-pgstac creates image tiles using rio-tiler which uses rasterio. Rasterio uses GDAL “under the hood”. Certain GDAL environment variables impact tiling performance when working with rasterio to read data from Cloud-Optimized GeoTIFFs\nAs noted in Benchmarking Methodolgy, the time to tile includes the time to query a pgSTAC database and then use the query ID returned to read and create image tiles from COGs on S3. The libraries used were pgSTAC for reading STAC metadata and rasterio (via rio_tiler) for reading COGs on S3.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables"
    ]
  },
  {
    "objectID": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html#explanation",
    "href": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html#explanation",
    "title": "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables",
    "section": "",
    "text": "This notebook does not report any results for tiling Zarr datasets. It surfaces the significance of the underlying environment and configuration of low level libraries to the performance of a framework we are comparing with for tiling imagery.\ntitiler-pgstac creates image tiles using rio-tiler which uses rasterio. Rasterio uses GDAL “under the hood”. Certain GDAL environment variables impact tiling performance when working with rasterio to read data from Cloud-Optimized GeoTIFFs\nAs noted in Benchmarking Methodolgy, the time to tile includes the time to query a pgSTAC database and then use the query ID returned to read and create image tiles from COGs on S3. The libraries used were pgSTAC for reading STAC metadata and rasterio (via rio_tiler) for reading COGs on S3.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables"
    ]
  },
  {
    "objectID": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html#dataset-generation",
    "href": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html#dataset-generation",
    "title": "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables",
    "section": "Dataset Generation",
    "text": "Dataset Generation\nAll dataset generation code is in the tiling-benchmark repo’s cmip6-pgstac directory. The STAC collection is defined in CMIP6_daily_GISS-E2-1-G_tas_collection.json. The STAC item records for the CMIP6 COGs are generated in the 01-generate-datasets/cmip6-pgstac/generate_cmip6_items.ipynb notebook. They are seeded via seed-db.sh.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables"
    ]
  },
  {
    "objectID": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html#tests",
    "href": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html#tests",
    "title": "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables",
    "section": "Tests",
    "text": "Tests\nTests were run via the tile-benchmarking/02-run-tests/01-cog-gdal-tests.ipynb notebook.\n\nimport pandas as pd\nimport warnings\nimport holoviews as hv\n\npd.options.plotting.backend = \"holoviews\"\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\n\n\n\n\n\n\n\ngit_url_path = \"https://raw.githubusercontent.com/developmentseed/tile-benchmarking/main/02-run-tests/results-csvs/\"\ndf = pd.read_csv(f\"{git_url_path}/01-cog-gdal-results.csv\")\ndf[\"set_gdal_vars\"] = df[\"set_gdal_vars\"].astype(str)\n\n\nzooms = range(6)\ncmap = [\"#E1BE6A\", \"#40B0A6\"]\nplt_opts = {\"width\": 300, \"height\": 250}\n\nplts = []\n\nfor zoom_level in zooms:\n    df_level = df[df[\"zoom\"] == zoom_level]\n    plts.append(\n        df.hvplot.box(\n            y=\"time\",\n            by=[\"set_gdal_vars\"],\n            c=\"set_gdal_vars\",\n            cmap=cmap,\n            ylabel=\"Time to render (ms)\",\n            xlabel=\"GDAL Environment Variables Set/Unset\",\n            legend=False,\n            title=f\"Zoom level {zoom_level}\",\n        ).opts(**plt_opts)\n    )\nhv.Layout(plts).cols(2)",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables"
    ]
  },
  {
    "objectID": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html#interpretation-of-the-results",
    "href": "approaches/tiling/01-cmip6-cog-tile-server-benchmarks.html#interpretation-of-the-results",
    "title": "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables",
    "section": "Interpretation of the Results",
    "text": "Interpretation of the Results\n\nSetting these GDAL environment variables significantly impacts performance, with 100x speed up in performance.\nNot shown above, but variation across tiles is not significant.\nVariation across zoom levels is not significant.\n\nThese GDAL variables are documented here: https://developmentseed.org/titiler/advanced/performance_tuning/.\nBy setting the GDAL environment variables we limit the number of total requests to S3. Specifically, these environment variables ensure that:\n\nAll of the metadata may be read in 1 request. This is not necessarily true, but more likely since we increase the initial number of GDAL ingested bytes.\nThere are no extra LIST requests which GDAL uses to discover sidecar files. COGs don’t have sidecar files.\nConsecutive range requests are merged into 1 request.\nMultiple range requests use the same TCP connection.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 1: Tiling COGs with and without GDAL Environment Variables"
    ]
  },
  {
    "objectID": "approaches/tiling/future-areas.html",
    "href": "approaches/tiling/future-areas.html",
    "title": "Future Areas",
    "section": "",
    "text": "Future Areas\nAt this time, we plan to develop the tile server approach in the following areas:\n\nImplement caching at the application level with zarr metadata caching.\nAdd support for unconsolidated metadata.\nAdd support for HDF5\nImplement caching at the API level with CloudFront.\n\nAt this time, the following areas of research and development are of interest but are unplanned:\n\nExacting guidelines on when to create pyramids. For example, if your dataset is x chunk shape and size and you expect y performance, you should or should not create z levels of pyramid.\nUnderstand the impact of latency and caching for layers of AWS services. We expect there may be some caching in the S3 service but have not tested or verified this.\nThere is latency introduced with the API Gateway and Lambda services. It may be helpful to estimate the latency for these layers and different expected dataset configurations.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Future Areas"
    ]
  },
  {
    "objectID": "approaches/tiling/03-zarr-chunk-sizes.html",
    "href": "approaches/tiling/03-zarr-chunk-sizes.html",
    "title": "Benchmarks 3: Chunk Size Variation",
    "section": "",
    "text": "We compared the performance of tiling artificially generated Zarr data to different chunk sizes. The CMIP6 data provides an excellent real world dataset, but is relatively low resolution. In order to study the impact of higher resolution data, we artificially generated Zarr datastores to explore the relationship between tile generation time and chunk size.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 3: Chunk Size Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/03-zarr-chunk-sizes.html#explanation",
    "href": "approaches/tiling/03-zarr-chunk-sizes.html#explanation",
    "title": "Benchmarks 3: Chunk Size Variation",
    "section": "",
    "text": "We compared the performance of tiling artificially generated Zarr data to different chunk sizes. The CMIP6 data provides an excellent real world dataset, but is relatively low resolution. In order to study the impact of higher resolution data, we artificially generated Zarr datastores to explore the relationship between tile generation time and chunk size.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 3: Chunk Size Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/03-zarr-chunk-sizes.html#dataset-generation",
    "href": "approaches/tiling/03-zarr-chunk-sizes.html#dataset-generation",
    "title": "Benchmarks 3: Chunk Size Variation",
    "section": "Dataset Generation",
    "text": "Dataset Generation\nWe generated multiple data stores of increasingly fine spatial resolution so the total size of each chunk, with no spatial chunks, is multiplied by a factor of 2, to allow for increased chunk size.\nThe code to produce the zarr stores are in the tile-benchmarking repo: 01-generate-datasets/generate-fake-data-with-chunks.ipynb.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 3: Chunk Size Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/03-zarr-chunk-sizes.html#tests",
    "href": "approaches/tiling/03-zarr-chunk-sizes.html#tests",
    "title": "Benchmarks 3: Chunk Size Variation",
    "section": "Tests",
    "text": "Tests\nTests were run via the tile-benchmarking/02-run-tests/03-chunk-size.ipynb notebook.\n\nimport pandas as pd\nimport warnings\nimport holoviews as hv\n\npd.options.plotting.backend = \"holoviews\"\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\n\n\n\n\n\n\n\ngit_url_path = \"https://raw.githubusercontent.com/developmentseed/tile-benchmarking/main/02-run-tests/results-csvs/\"\ndf = pd.read_csv(f\"{git_url_path}/03-chunk-size-results.csv\")\n\n\nzooms = range(6)\nplt_opts = {\"width\": 400, \"height\": 300}\n\nplts = []\n\nfor zoom_level in zooms:\n    df_level = df[df[\"zoom\"] == zoom_level]\n    plts.append(\n        df_level.hvplot.box(\n            y=\"time\",\n            by=[\"chunk_size_mb\"],\n            c=\"chunk_size_mb\",\n            cmap=\"Plasma_r\",\n            ylabel=\"Time to render (ms)\",\n            xlabel=\"Chunk size (MB)\",\n            legend=False,\n            title=f\"Zoom level {zoom_level}\",\n        ).opts(**plt_opts)\n    )\nhv.Layout(plts).cols(2)",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 3: Chunk Size Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/03-zarr-chunk-sizes.html#interpretation-of-the-results",
    "href": "approaches/tiling/03-zarr-chunk-sizes.html#interpretation-of-the-results",
    "title": "Benchmarks 3: Chunk Size Variation",
    "section": "Interpretation of the Results",
    "text": "Interpretation of the Results\nIt’s clear smaller chunk sizes support faster tile rendering, regardless of zoom level. However, the tradeoff will be in the number of chunks. Smaller chunk sizes means more spatial chunks will be required to render the data which can impact performance at low zoom levels. We will take a look at this in the next notebook Benchmarks: Exploring Spatial Chunk Variation Impacts on Tile Generation.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 3: Chunk Size Variation"
    ]
  },
  {
    "objectID": "approaches/tiling/05-cmip6-pyramids.html",
    "href": "approaches/tiling/05-cmip6-pyramids.html",
    "title": "Benchmarks 5: Zarr Pyramids",
    "section": "",
    "text": "In the previous notebooks, we saw how chunk size and number can impact performance. Pyramids, or multiscale datasets, aggregate data at various levels to create reduced resolution datasets which perform better at low zoom levels. These datasets are not representing the “raw data” but it is assumed these aggregated datasets are intended for visual representation of the data and not numerical analysis.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 5: Zarr Pyramids"
    ]
  },
  {
    "objectID": "approaches/tiling/05-cmip6-pyramids.html#explanation",
    "href": "approaches/tiling/05-cmip6-pyramids.html#explanation",
    "title": "Benchmarks 5: Zarr Pyramids",
    "section": "",
    "text": "In the previous notebooks, we saw how chunk size and number can impact performance. Pyramids, or multiscale datasets, aggregate data at various levels to create reduced resolution datasets which perform better at low zoom levels. These datasets are not representing the “raw data” but it is assumed these aggregated datasets are intended for visual representation of the data and not numerical analysis.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 5: Zarr Pyramids"
    ]
  },
  {
    "objectID": "approaches/tiling/05-cmip6-pyramids.html#dataset-generation",
    "href": "approaches/tiling/05-cmip6-pyramids.html#dataset-generation",
    "title": "Benchmarks 5: Zarr Pyramids",
    "section": "Dataset Generation",
    "text": "Dataset Generation\nThe code to produce the zarr pyramids is in the tile-benchmarking repo: 02-run-tests/05-cmip6-pyramid.ipynb.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 5: Zarr Pyramids"
    ]
  },
  {
    "objectID": "approaches/tiling/05-cmip6-pyramids.html#tests",
    "href": "approaches/tiling/05-cmip6-pyramids.html#tests",
    "title": "Benchmarks 5: Zarr Pyramids",
    "section": "Tests",
    "text": "Tests\nTests were run via the tile-benchmarking/02-run-tests/04-number-of-spatial-chunks.ipynb notebook.\n\nimport pandas as pd\nimport warnings\nimport holoviews as hv\n\npd.options.plotting.backend = \"holoviews\"\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\n\n\n\n\n\n\n\ngit_url_path = \"https://raw.githubusercontent.com/developmentseed/tile-benchmarking/main/02-run-tests/results-csvs/\"\ndf = pd.read_csv(f\"{git_url_path}/05-cmip6-pyramid-results.csv\")\n\n\nzooms = range(4)\n\nplt_opts = {\"width\": 400, \"height\": 300}\n\nplts = []\n\nfor zoom_level in zooms:\n    df_level = df[df[\"zoom\"] == zoom_level]\n    plts.append(\n        df_level.hvplot.box(\n            y=\"time\",\n            by=[\"data_format\"],\n            c=\"data_format\",\n            cmap=\"Plasma_r\",\n            ylabel=\"Time to render (ms)\",\n            xlabel=\"Data Format\",\n            legend=False,\n            title=f\"Zoom level {zoom_level}\",\n        ).opts(**plt_opts)\n    )\nhv.Layout(plts).cols(2)",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 5: Zarr Pyramids"
    ]
  },
  {
    "objectID": "approaches/tiling/05-cmip6-pyramids.html#interpretation-of-the-results",
    "href": "approaches/tiling/05-cmip6-pyramids.html#interpretation-of-the-results",
    "title": "Benchmarks 5: Zarr Pyramids",
    "section": "Interpretation of the Results",
    "text": "Interpretation of the Results\nAt low zoom levels, pyramids improved performance. It is notable that this dataset is not very high resolution and that a more significant performance improvement may be seen for pyramids for higher resolution datasets. For the CMIP6 dataset, it only makes sense to use a pyramid for zoom levels 0 and 1.",
    "crumbs": [
      "Approaches",
      "Tiling",
      "Benchmarks 5: Zarr Pyramids"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/recommendations.html",
    "href": "approaches/dynamic-client/recommendations.html",
    "title": "Recommendations",
    "section": "",
    "text": "Here we provide recommendations for producing pyramids for performant Zarr visualization on the web. These recommendations are based on the end-to-end benchmarking results for the dynamic client approach. These benchmark consider the use-case of rendering data on a map. However, we also discuss how time series visualization could factor into these decisions. We eventually aim to make pyramid generation optional for the dynamic client approach, such that the raw data could be rendered on a web map with limited zooming and panning functionality. Many of these recommendations should still apply when rendering raw data on a web map, however, it would be much more important to also consider the performance implications for scientific computational workflows in that case.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Recommendations"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/recommendations.html#zarr-version",
    "href": "approaches/dynamic-client/recommendations.html#zarr-version",
    "title": "Recommendations",
    "section": "Zarr Version",
    "text": "Zarr Version\nThe end-to-end benchmarking results showed that V2 and V3 data are comparable in performance. Therefore, we recommend adopting the Zarr V3 specification if your preferred Zarr implementation includes the approved version of the Zarr V3 spec. At the time of writing, the Zarrita Python library has implemented the approved Zarr V3 spec but is not recommended for production use. The Zarr Python library is undergoing a refactor to bring the library up-to-date with the V3 spec.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Recommendations"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/recommendations.html#number-of-pixels-per-tile-spatial-chunking",
    "href": "approaches/dynamic-client/recommendations.html#number-of-pixels-per-tile-spatial-chunking",
    "title": "Recommendations",
    "section": "Number of pixels per tile (spatial chunking)",
    "text": "Number of pixels per tile (spatial chunking)\nThe number of pixels per tile (i.e., spatial chunking) must be a multiple of 16 and is generally 128, 256, or 512. The end-to-end benchmarks tested 128 and 256 pixels per tile and showed that the number of pixels per tile does not impact rendering performance at a given chunk size. However, including more pixels per tile would reduce the proportion of other dimensions (e.g., time) that can be included in a chunk of a given size. Therefore, it would be worth considering fewer pixels per tile (e.g., 128) if visualizing time series is an important use case. By contrast, if only spatial rendering is important and more detail at coarser zoom levels is desired, you may consider increasing the number of pixels per tile to 256. If corresponding to “de facto” standards is particularly important, 256 is commonly used as the spatial width for tiles. In the current implementation of the dynamic client approach, increasing the number of pixels per tile does not increase total storage costs, as the number of zoom levels before reaching full resolution would be correspondingly smaller. However, larger chunk sizes could reduce storage costs when the requirement to align chunks with conventional zoom level boundaries is relaxed in future releases of the maps library.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Recommendations"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/recommendations.html#chunk-size-non-spatial-chunking",
    "href": "approaches/dynamic-client/recommendations.html#chunk-size-non-spatial-chunking",
    "title": "Recommendations",
    "section": "Chunk size (non-spatial chunking)",
    "text": "Chunk size (non-spatial chunking)\nThe chunk size was the strongest driver for the total time required to render datasets. For optimal rendering performance, we recommended targeting chunk sizes &lt;1MB for the uncompressed data.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Recommendations"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/recommendations.html#zarr-v3-sharding-extension",
    "href": "approaches/dynamic-client/recommendations.html#zarr-v3-sharding-extension",
    "title": "Recommendations",
    "section": "Zarr V3 sharding extension",
    "text": "Zarr V3 sharding extension\nThe end-to-end benchmarks showed that the time to render was slower for sharded V3 datasets relative to V2 datasets for zoom levels greater than 0. However, a primary benefit of the sharding extension is that it allows the same dataset to be accessed via large shards for analysis and smaller chunks for visualization. Further, a single file can store many chunks which benefits applications that rely on file-based operations. Given these benefits, we recommend leveraging the sharding specification after its review and acceptance as a Zarr Enhancement Proposal (ZEP) and your preferred Zarr implementation includes the approved sharding extension. The voting process for this ZEP expected to end on October 31, 2023. We found the the shard size does not impact the time to render and therefore recommend a follow-up study on optimal shard structures for computational workflows.\nWe expect that the performance difference between sharded and non-sharded datasets could be minimized by future optimizations in the loading library, such as through the concatenation of range requests for adjacent chunks at higher zoom levels.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Recommendations"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/e2e-results-pixels-per-tile.html",
    "href": "approaches/dynamic-client/e2e-results-pixels-per-tile.html",
    "title": "Benchmarking: Pixels per Tile",
    "section": "",
    "text": "import hvplot\nimport hvplot.pandas  # noqa\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\n\n\n\n\n\n\nRead summary of all benchmarking results.\n\nsummary = pd.read_parquet(\"s3://carbonplan-benchmarks/benchmark-data/v0.2/summary.parq\")\n\nSubset the data to isolate the impact of the number of pixels per tile and chunk size.\n\ndf = summary[\n    (summary[\"projection\"] == 3857) & (summary[\"region\"] == \"us-west-2\")\n].sort_values(by=[\"target_chunk_size\", \"pixels_per_tile\"])\n\nSet plot options.\n\ncmap = [\"#994F00\", \"#006CD1\"]\nplt_opts = {\"width\": 600, \"height\": 400}\n\nCreate a box plot showing how the rendering time depends on the number of pixels per tile and chunk size.\n\ndf.hvplot.box(\n    y=\"duration\",\n    by=[\"actual_chunk_size\", \"pixels_per_tile\"],\n    c=\"pixels_per_tile\",\n    cmap=cmap,\n    ylabel=\"Time to render (ms)\",\n    xlabel=\"Chunk size (MB); Pixels per tile\",\n    legend=False,\n).opts(**plt_opts)\n\n\n\n\n\n  \n\n\n\n\nFit a multiple linear regression to the results. The results show that the number of pixels per tile independent of the chunk size does not significantly impact rendering time. Datasets with larger chunks take longer to render.\n\nmodel = smf.ols(\"duration ~ actual_chunk_size + C(pixels_per_tile)\", data=df).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nduration\nR-squared:\n0.275\n\n\nModel:\nOLS\nAdj. R-squared:\n0.273\n\n\nMethod:\nLeast Squares\nF-statistic:\n193.4\n\n\nDate:\nTue, 29 Aug 2023\nProb (F-statistic):\n6.08e-72\n\n\nTime:\n20:29:05\nLog-Likelihood:\n-7981.0\n\n\nNo. Observations:\n1024\nAIC:\n1.597e+04\n\n\nDf Residuals:\n1021\nBIC:\n1.598e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2031.8289\n33.974\n59.805\n0.000\n1965.162\n2098.496\n\n\nC(pixels_per_tile)[T.256]\n-3.3655\n37.576\n-0.090\n0.929\n-77.101\n70.370\n\n\nactual_chunk_size\n43.2144\n2.250\n19.209\n0.000\n38.800\n47.629\n\n\n\n\n\n\n\n\nOmnibus:\n35.870\nDurbin-Watson:\n2.052\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n39.124\n\n\nSkew:\n0.477\nProb(JB):\n3.19e-09\n\n\nKurtosis:\n2.907\nCond. No.\n29.2\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking: Pixels per Tile"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/costs.html",
    "href": "approaches/dynamic-client/costs.html",
    "title": "Dynamic Client: Costs",
    "section": "",
    "text": "The primary categories of costs associated with the dynamic client approach are post-processing costs, data storage costs, and data egress costs.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Dynamic Client: Costs"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/costs.html#overview",
    "href": "approaches/dynamic-client/costs.html#overview",
    "title": "Dynamic Client: Costs",
    "section": "",
    "text": "The primary categories of costs associated with the dynamic client approach are post-processing costs, data storage costs, and data egress costs.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Dynamic Client: Costs"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/costs.html#post-processing-costs",
    "href": "approaches/dynamic-client/costs.html#post-processing-costs",
    "title": "Dynamic Client: Costs",
    "section": "Post-processing costs",
    "text": "Post-processing costs\nThe post-processing costs are associated with the time required to generate pyramids for rendering. As the post-processing costs should be reduced by planned performance improvements to ndpyramid and the generation of pyramids during dataset creation, we do not provide specific estimates for the post-processing costs in this cookbook.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Dynamic Client: Costs"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/costs.html#data-storage-costs",
    "href": "approaches/dynamic-client/costs.html#data-storage-costs",
    "title": "Dynamic Client: Costs",
    "section": "Data storage costs",
    "text": "Data storage costs\nThe data storage costs refer to the increased costs associated with storing pyramids along with the original data. For context, current S3 pricing is detailed at https://aws.amazon.com/s3/pricing/. The rates vary based on the the object size and the storage class; we consider the average rate in the widget below for simplicity.\nFor datasets with non-dimensional coordinates and a single non-spatial dimension, the cost associated with storing coordinates and metadata is negligible compared to the storage costs of the variable data.\nThe data type can dramatically affect the storage costs. For example, increasing the precision from a single precision to a double precision float or reducing to a half precision would double or half the storage costs respectively, again treating coordinate and metadata storage as negligible. It is important to consider whether the selected data type is supported by all applications that will use the data.\nThe compression can also dramatically impact the storage costs. The benchmarks in this experiment all used gzip level 1 compression, single precision floats for the variable data and spatial coordinates, and integers for the time dimension. Future work should consider the impact of data type, compression, and bitrounding on performance, as these variables have the potential to dramatically reduce costs.\nData storage costs also be reduced by leveraging the Zarr feature that allows not writing chunks comprised only of the specified fill value. For example, this would allow not writing chunks that correspond to oceans for a global, land-based dataset, yielding tremendous savings at high zoom levels. The primary downside is less confidence that all non-empty chunks were written, as the reason for not writing specific chunks is not currently stored in Zarr metadata.\nThe small interactive application below shows the storage costs for different data configurations. The tool explores storage costs for global pyramids generated using ndpyramid’s pyramid_reproject or pyramid_regrid methods for the @carbonplan/maps library. Future iterations on this tool should explore costs of coarsened pyramids for use in other visualization libraries. Only a small portion of the parameter space is included in the embedded application; please download this notebook to fully explore the exercise. The application shows the dramatic impact of the number of zoom levels included on storage costs.\n\nimport panel as pn\nfrom cost_widgets import (\n    calculate_pyramid_cost,\n    compression_widget,\n    dtype_widget,\n    extra_dim_widget,\n    price_widget,\n    zoom_level_widget,\n)\n\npn.extension()\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n# Create panel app\nbound_disp = pn.bind(\n    calculate_pyramid_cost,\n    number_of_zoom_levels=zoom_level_widget,\n    pixels_per_tile=128,\n    extra_dimension_length=extra_dim_widget,\n    data_dtype=dtype_widget,\n    data_compression_ratio=compression_widget,\n    price_per_GB=price_widget,\n)\npn.Column(\n    bound_disp,\n    dtype_widget,\n    zoom_level_widget,\n    extra_dim_widget,\n    compression_widget,\n    price_widget,\n).embed()",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Dynamic Client: Costs"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/costs.html#data-request-and-egress-costs",
    "href": "approaches/dynamic-client/costs.html#data-request-and-egress-costs",
    "title": "Dynamic Client: Costs",
    "section": "Data request and egress costs",
    "text": "Data request and egress costs\nData egress costs are associated with serving data from a cloud storage location to the client. Typically, the costs are based on the amount of data transferred from the cloud storage location to the internet, the number of requests made against the buckets and objects, and the location that the data will be transferred to. For specific estimations of AWS pricing for data requests and transfer, see https://aws.amazon.com/s3/pricing/. Currently, the first 100GB each month are covered by AWS’s free tier, which additional GB charged at a rate per GB with the rate incrementally decreasing at several thresholds.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Dynamic Client: Costs"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/e2e-results.html",
    "href": "approaches/dynamic-client/e2e-results.html",
    "title": "Benchmarking: Individual Results",
    "section": "",
    "text": "The CarbonPlan team put together some utilities for parsing, processing, and visualizing the benchmarking results in carbonplan_benchmarks. We’ll use those utilities along with the Holoviz suite of tools for visualization to show some individual benchmarking results.\n\nimport carbonplan_benchmarks.analysis as cba\nimport pandas as pd\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefine URIs for the baseline images and metadata\n\nbaseline_fp = \"s3://carbonplan-benchmarks/benchmark-data/v0.2/baselines.json\"\nurl_filter = \"carbonplan-benchmarks.s3.us-west-2.amazonaws.com/data/\"\nmetadata_fp = (\n    \"s3://carbonplan-benchmarks/benchmark-data/v0.2/data-2023-08-24T00-33-44.json\"\n)\n\nDefine some plot options\n\nplt_opts = {\"width\": 500, \"height\": 200}\n\nLoad the data and baseline images\n\nmetadata, trace_events = cba.load_data(metadata_path=metadata_fp, run=0)\nsnapshots = cba.load_snapshots(snapshot_path=baseline_fp)\n\nProcess the data\n\ndata = cba.process_run(\n    metadata=metadata, trace_events=trace_events, snapshots=snapshots\n)\n\nCreate summary statistics\n\ncba.create_summary(metadata=metadata, data=data, url_filter=url_filter)\n\n\n\n\n\n\n\n\naction\napproach\nbenchmark_version\nbrowser_name\nbrowser_version\nheadless\nplaywright_python_version\nprovider\nrun_number\ntimeout\n...\nfiltered_requests_average_encoded_data_length\nfiltered_requests_maximum_encoded_data_length\nzoom\nduration\nmin_rmse\nfps\nrequest_duration\nrequest_percent\nnon_request_duration\nactual_chunk_size\n\n\ndataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyramids-v2-3857-True-256-1-0-0-f4-0-0-0-gzipL1-100\nzoom_in\ndynamic-client\nv0.2\nchromium\n116.0.5845.82\nTrue\n1.37.0\naws\n1\nFalse\n...\n63488.00\n125345.0\n0.0\n2313.558\n0.0\n59.648386\n408.764\n17.668198\n1904.794\n0.524288\n\n\npyramids-v2-3857-True-256-1-0-0-f4-0-0-0-gzipL1-100\nzoom_in\ndynamic-client\nv0.2\nchromium\n116.0.5845.82\nTrue\n1.37.0\naws\n1\nFalse\n...\n111303.25\n206084.0\n1.0\n2468.501\n0.0\n60.360518\n1365.362\n55.311381\n1103.139\n0.524288\n\n\npyramids-v2-3857-True-256-1-0-0-f4-0-0-0-gzipL1-100\nzoom_in\ndynamic-client\nv0.2\nchromium\n116.0.5845.82\nTrue\n1.37.0\naws\n1\nFalse\n...\n157726.00\n342805.0\n2.0\n1591.088\n0.0\n60.336072\n987.093\n62.038869\n603.995\n0.524288\n\n\npyramids-v2-3857-True-256-1-0-0-f4-0-0-0-gzipL1-100\nzoom_in\ndynamic-client\nv0.2\nchromium\n116.0.5845.82\nTrue\n1.37.0\naws\n1\nFalse\n...\n164329.00\n334586.0\n3.0\n1229.950\n0.0\n59.352006\n593.159\n48.226269\n636.791\n0.524288\n\n\n\n\n4 rows × 34 columns\n\n\n\nPlot the requests, frames, and baseline comparisons\n\nrequests_plt = cba.plot_requests(data[\"request_data\"], url_filter=url_filter).opts(\n    **plt_opts\n)\nframes_plt = cba.plot_frames(data[\"frames_data\"], yl=2.5).opts(**plt_opts)\nrmse_plt = cba.plot_screenshot_rmse(\n    screenshot_data=data[\"screenshot_data\"], metadata=metadata\n).opts(**plt_opts)\n(requests_plt + frames_plt + rmse_plt).cols(1)\n\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_drag' property; using the latest value\n  layout_plot = gridplot(\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_scroll' property; using the latest value\n  layout_plot = gridplot(",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking: Individual Results"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/e2e-results.html#processing-benchmark-results",
    "href": "approaches/dynamic-client/e2e-results.html#processing-benchmark-results",
    "title": "Benchmarking: Individual Results",
    "section": "",
    "text": "The CarbonPlan team put together some utilities for parsing, processing, and visualizing the benchmarking results in carbonplan_benchmarks. We’ll use those utilities along with the Holoviz suite of tools for visualization to show some individual benchmarking results.\n\nimport carbonplan_benchmarks.analysis as cba\nimport pandas as pd\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefine URIs for the baseline images and metadata\n\nbaseline_fp = \"s3://carbonplan-benchmarks/benchmark-data/v0.2/baselines.json\"\nurl_filter = \"carbonplan-benchmarks.s3.us-west-2.amazonaws.com/data/\"\nmetadata_fp = (\n    \"s3://carbonplan-benchmarks/benchmark-data/v0.2/data-2023-08-24T00-33-44.json\"\n)\n\nDefine some plot options\n\nplt_opts = {\"width\": 500, \"height\": 200}\n\nLoad the data and baseline images\n\nmetadata, trace_events = cba.load_data(metadata_path=metadata_fp, run=0)\nsnapshots = cba.load_snapshots(snapshot_path=baseline_fp)\n\nProcess the data\n\ndata = cba.process_run(\n    metadata=metadata, trace_events=trace_events, snapshots=snapshots\n)\n\nCreate summary statistics\n\ncba.create_summary(metadata=metadata, data=data, url_filter=url_filter)\n\n\n\n\n\n\n\n\naction\napproach\nbenchmark_version\nbrowser_name\nbrowser_version\nheadless\nplaywright_python_version\nprovider\nrun_number\ntimeout\n...\nfiltered_requests_average_encoded_data_length\nfiltered_requests_maximum_encoded_data_length\nzoom\nduration\nmin_rmse\nfps\nrequest_duration\nrequest_percent\nnon_request_duration\nactual_chunk_size\n\n\ndataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyramids-v2-3857-True-256-1-0-0-f4-0-0-0-gzipL1-100\nzoom_in\ndynamic-client\nv0.2\nchromium\n116.0.5845.82\nTrue\n1.37.0\naws\n1\nFalse\n...\n63488.00\n125345.0\n0.0\n2313.558\n0.0\n59.648386\n408.764\n17.668198\n1904.794\n0.524288\n\n\npyramids-v2-3857-True-256-1-0-0-f4-0-0-0-gzipL1-100\nzoom_in\ndynamic-client\nv0.2\nchromium\n116.0.5845.82\nTrue\n1.37.0\naws\n1\nFalse\n...\n111303.25\n206084.0\n1.0\n2468.501\n0.0\n60.360518\n1365.362\n55.311381\n1103.139\n0.524288\n\n\npyramids-v2-3857-True-256-1-0-0-f4-0-0-0-gzipL1-100\nzoom_in\ndynamic-client\nv0.2\nchromium\n116.0.5845.82\nTrue\n1.37.0\naws\n1\nFalse\n...\n157726.00\n342805.0\n2.0\n1591.088\n0.0\n60.336072\n987.093\n62.038869\n603.995\n0.524288\n\n\npyramids-v2-3857-True-256-1-0-0-f4-0-0-0-gzipL1-100\nzoom_in\ndynamic-client\nv0.2\nchromium\n116.0.5845.82\nTrue\n1.37.0\naws\n1\nFalse\n...\n164329.00\n334586.0\n3.0\n1229.950\n0.0\n59.352006\n593.159\n48.226269\n636.791\n0.524288\n\n\n\n\n4 rows × 34 columns\n\n\n\nPlot the requests, frames, and baseline comparisons\n\nrequests_plt = cba.plot_requests(data[\"request_data\"], url_filter=url_filter).opts(\n    **plt_opts\n)\nframes_plt = cba.plot_frames(data[\"frames_data\"], yl=2.5).opts(**plt_opts)\nrmse_plt = cba.plot_screenshot_rmse(\n    screenshot_data=data[\"screenshot_data\"], metadata=metadata\n).opts(**plt_opts)\n(requests_plt + frames_plt + rmse_plt).cols(1)\n\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_drag' property; using the latest value\n  layout_plot = gridplot(\n/Users/max/mambaforge/envs/benchmark-maps/lib/python3.10/site-packages/holoviews/plotting/bokeh/plot.py:987: UserWarning: found multiple competing values for 'toolbar.active_scroll' property; using the latest value\n  layout_plot = gridplot(",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking: Individual Results"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/e2e-results-shard-size.html",
    "href": "approaches/dynamic-client/e2e-results-shard-size.html",
    "title": "Benchmarking: Shard Size",
    "section": "",
    "text": "import hvplot\nimport hvplot.pandas  # noqa\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\npd.options.plotting.backend = \"holoviews\"\n\n\n\n\n\n\n\n\n\n\nRead summary of all benchmarking results.\n\nsummary = pd.read_parquet(\"s3://carbonplan-benchmarks/benchmark-data/v0.2/summary.parq\")\n\nSubset the data to isolate the impact of chunk and shard size.\n\ndf = summary[\n    (summary[\"projection\"] == 4326)\n    & (summary[\"pixels_per_tile\"] == 128)\n    & (summary[\"shard_size\"] &gt; 0)\n    & (summary[\"region\"] == \"us-west-2\")\n]\n\nCreate a box plot showing how the rendering time depends on chunk and shard size.\n\ndf.hvplot.box(\n    y=\"duration\",\n    by=[\"actual_chunk_size\", \"shard_size\"],\n    c=\"shard_size\",\n    cmap=[\"#FEFE62\", \"#D35FB7\"],\n    ylabel=\"Time to render (ms)\",\n    xlabel=\"Chunk size (MB); Target shard size (MB)\",\n    legend=False,\n).opts(width=600, height=400)\n\n\n\n\n\n  \n\n\n\n\nFit a multiple linear regression to the results. The results show that the chunk size strongly impacts the time to render. Datasets with larger chunk sizes take longer to render. The shard size does not have a noticeable impact on rendering time.\n\nmodel = smf.ols(\"duration ~ actual_chunk_size + shard_size\", data=df).fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nduration\nR-squared:\n0.398\n\n\nModel:\nOLS\nAdj. R-squared:\n0.393\n\n\nMethod:\nLeast Squares\nF-statistic:\n83.71\n\n\nDate:\nTue, 29 Aug 2023\nProb (F-statistic):\n1.25e-28\n\n\nTime:\n20:30:46\nLog-Likelihood:\n-2063.2\n\n\nNo. Observations:\n256\nAIC:\n4132.\n\n\nDf Residuals:\n253\nBIC:\n4143.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2446.8248\n161.251\n15.174\n0.000\n2129.259\n2764.391\n\n\nactual_chunk_size\n70.9110\n5.482\n12.935\n0.000\n60.115\n81.707\n\n\nshard_size\n0.6188\n1.925\n0.321\n0.748\n-3.172\n4.409\n\n\n\n\n\n\n\n\nOmnibus:\n31.868\nDurbin-Watson:\n2.302\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n39.758\n\n\nSkew:\n-0.911\nProb(JB):\n2.33e-09\n\n\nKurtosis:\n3.637\nCond. No.\n267.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Benchmarking: Shard Size"
    ]
  },
  {
    "objectID": "approaches/dynamic-client/future-areas.html",
    "href": "approaches/dynamic-client/future-areas.html",
    "title": "Future Areas",
    "section": "",
    "text": "The benchmarking results and recommendations detailed in this cookbook motivates several possible avenues of exploration:\n\nPerformance improvements for ndpyramid to reduce post-processing costs.\nGeneralize ndpyramid to better support other mapping approaches.\nIntegration between ndpyramid and pangeo-forge-recipes to support generating pyramids during dataset creation.\nExtending the dynamic client approach to visualize datasets without pyramids.\nFormalizing the representation of pyramids and spatial overviews through a Zarr Enhancement Proposal (ZEP) on multiscales.\nExtending the dynamic client approach to visualize COGs using Kerchunk.\nGeneralizing the dynamic client approach as a plug-in for general solutions like Mapbox or deck.gl.\nSupporting and benchmarking additional compression algorithms, data types, and bitrounding.",
    "crumbs": [
      "Approaches",
      "Dynamic Client",
      "Future Areas"
    ]
  }
]